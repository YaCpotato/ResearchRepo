[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 1807640410312051969
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 1070737818776708113
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:1"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 12982020711197788453
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:2"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 4119727959816121161
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:3"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 7931899649767476064
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 14054028182770737464
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 76460321335508119
physical_device_desc: "device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3d:00.0, compute capability: 7.0"
, name: "/device:GPU:1"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 1129041477695652492
physical_device_desc: "device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3e:00.0, compute capability: 7.0"
, name: "/device:GPU:2"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 1290657136344883007
physical_device_desc: "device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b1:00.0, compute capability: 7.0"
, name: "/device:GPU:3"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 7902829521905077210
physical_device_desc: "device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b2:00.0, compute capability: 7.0"
]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 160)  0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 160)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 160)  0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 160)  0           activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16, 16, 320)  0           activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 16, 16, 320)  0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      dropout_6[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 16, 320)  0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      dropout_7[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 16, 16, 320)  0           activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 8, 8, 640)    0           activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 8, 8, 640)    0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 8, 8, 640)    0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 8, 8, 640)    0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           6410        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,507,242
Trainable params: 36,489,290
Non-trainable params: 17,952
__________________________________________________________________________________________________
Train on 45000 samples, validate on 5000 samples
Epoch 1/100
 - 79s - loss: 3.9886 - acc: 0.2994 - val_loss: 1.8972 - val_acc: 0.3632
Epoch 2/100
 - 70s - loss: 1.5466 - acc: 0.4722 - val_loss: 1.9551 - val_acc: 0.3542
Epoch 3/100
 - 70s - loss: 1.3163 - acc: 0.5455 - val_loss: 1.4322 - val_acc: 0.5150
Epoch 4/100
 - 70s - loss: 1.1484 - acc: 0.5996 - val_loss: 1.1747 - val_acc: 0.5848
Epoch 5/100
 - 70s - loss: 0.9858 - acc: 0.6488 - val_loss: 1.2312 - val_acc: 0.5898
Epoch 6/100
 - 70s - loss: 0.8726 - acc: 0.6911 - val_loss: 1.1032 - val_acc: 0.6326
Epoch 7/100
 - 70s - loss: 0.7897 - acc: 0.7204 - val_loss: 0.9882 - val_acc: 0.6588
Epoch 8/100
 - 70s - loss: 0.7189 - acc: 0.7459 - val_loss: 0.8787 - val_acc: 0.7018
Epoch 9/100
 - 70s - loss: 0.6588 - acc: 0.7662 - val_loss: 1.5214 - val_acc: 0.5578
Epoch 10/100
 - 70s - loss: 0.6084 - acc: 0.7854 - val_loss: 0.9173 - val_acc: 0.6854
Epoch 11/100
 - 70s - loss: 0.5622 - acc: 0.8026 - val_loss: 0.8768 - val_acc: 0.7112
Epoch 12/100
 - 70s - loss: 0.5200 - acc: 0.8176 - val_loss: 0.8562 - val_acc: 0.7190
Epoch 13/100
 - 70s - loss: 0.4850 - acc: 0.8304 - val_loss: 0.7636 - val_acc: 0.7588
Epoch 14/100
 - 70s - loss: 0.4458 - acc: 0.8443 - val_loss: 0.7006 - val_acc: 0.7680
Epoch 15/100
 - 70s - loss: 0.4112 - acc: 0.8577 - val_loss: 1.1006 - val_acc: 0.6626
Epoch 16/100
 - 70s - loss: 0.3780 - acc: 0.8691 - val_loss: 0.7319 - val_acc: 0.7728
Epoch 17/100
 - 70s - loss: 0.3557 - acc: 0.8740 - val_loss: 0.7893 - val_acc: 0.7462
Epoch 18/100
 - 70s - loss: 0.3307 - acc: 0.8841 - val_loss: 0.6634 - val_acc: 0.7922
Epoch 19/100
 - 70s - loss: 0.3050 - acc: 0.8941 - val_loss: 0.8325 - val_acc: 0.7520
Epoch 20/100
 - 70s - loss: 0.2791 - acc: 0.9029 - val_loss: 0.7565 - val_acc: 0.7818
Epoch 21/100
 - 70s - loss: 0.2545 - acc: 0.9133 - val_loss: 0.6733 - val_acc: 0.7902
Epoch 22/100
 - 70s - loss: 0.2365 - acc: 0.9182 - val_loss: 1.0476 - val_acc: 0.7334
Epoch 23/100
 - 70s - loss: 0.2170 - acc: 0.9251 - val_loss: 0.7597 - val_acc: 0.7738
Epoch 24/100
 - 70s - loss: 0.1964 - acc: 0.9327 - val_loss: 0.9660 - val_acc: 0.7470
Epoch 25/100
 - 70s - loss: 0.1798 - acc: 0.9392 - val_loss: 0.9002 - val_acc: 0.7574
Epoch 26/100
 - 70s - loss: 0.1635 - acc: 0.9461 - val_loss: 0.6874 - val_acc: 0.8070
Epoch 27/100
 - 70s - loss: 0.1505 - acc: 0.9484 - val_loss: 0.8968 - val_acc: 0.7750
Epoch 28/100
 - 70s - loss: 0.1363 - acc: 0.9542 - val_loss: 0.9310 - val_acc: 0.7716
Epoch 29/100
 - 70s - loss: 0.1243 - acc: 0.9594 - val_loss: 0.7379 - val_acc: 0.8134
Epoch 30/100
 - 70s - loss: 0.1090 - acc: 0.9647 - val_loss: 0.7962 - val_acc: 0.7970
Epoch 31/100
 - 70s - loss: 0.0992 - acc: 0.9675 - val_loss: 0.7184 - val_acc: 0.8194
Epoch 32/100
 - 70s - loss: 0.0946 - acc: 0.9685 - val_loss: 0.8564 - val_acc: 0.7864
Epoch 33/100
 - 70s - loss: 0.0845 - acc: 0.9735 - val_loss: 0.8013 - val_acc: 0.8012
Epoch 34/100
 - 70s - loss: 0.0765 - acc: 0.9761 - val_loss: 1.3849 - val_acc: 0.7160
Epoch 35/100
 - 70s - loss: 0.0708 - acc: 0.9779 - val_loss: 0.9845 - val_acc: 0.7774
Epoch 36/100
 - 70s - loss: 0.0620 - acc: 0.9813 - val_loss: 0.7660 - val_acc: 0.8234
Epoch 37/100
 - 70s - loss: 0.0583 - acc: 0.9815 - val_loss: 0.7768 - val_acc: 0.8160
Epoch 38/100
 - 70s - loss: 0.0529 - acc: 0.9839 - val_loss: 1.1191 - val_acc: 0.7774
Epoch 39/100
 - 70s - loss: 0.0494 - acc: 0.9852 - val_loss: 0.9585 - val_acc: 0.7988
Epoch 40/100
 - 70s - loss: 0.0497 - acc: 0.9854 - val_loss: 0.8632 - val_acc: 0.8120
Epoch 41/100
 - 70s - loss: 0.0442 - acc: 0.9871 - val_loss: 0.8654 - val_acc: 0.8124
Epoch 42/100
 - 70s - loss: 0.0407 - acc: 0.9887 - val_loss: 0.9513 - val_acc: 0.8054
Epoch 43/100
 - 70s - loss: 0.0401 - acc: 0.9881 - val_loss: 0.8748 - val_acc: 0.8172
Epoch 44/100
 - 70s - loss: 0.0364 - acc: 0.9897 - val_loss: 1.0279 - val_acc: 0.7878
Epoch 45/100
 - 70s - loss: 0.0339 - acc: 0.9901 - val_loss: 0.8323 - val_acc: 0.8208
Epoch 46/100
 - 70s - loss: 0.0332 - acc: 0.9901 - val_loss: 0.9955 - val_acc: 0.7998
Epoch 47/100
 - 70s - loss: 0.0314 - acc: 0.9909 - val_loss: 1.1332 - val_acc: 0.7856
Epoch 48/100
 - 70s - loss: 0.0296 - acc: 0.9919 - val_loss: 0.7381 - val_acc: 0.8384
Epoch 49/100
 - 70s - loss: 0.0280 - acc: 0.9923 - val_loss: 1.1901 - val_acc: 0.7874
Epoch 50/100
 - 70s - loss: 0.0270 - acc: 0.9927 - val_loss: 1.0201 - val_acc: 0.7962
Epoch 51/100
 - 70s - loss: 0.0251 - acc: 0.9937 - val_loss: 1.0270 - val_acc: 0.8062
Epoch 52/100
 - 70s - loss: 0.0249 - acc: 0.9926 - val_loss: 0.8489 - val_acc: 0.8212
Epoch 53/100
 - 70s - loss: 0.0222 - acc: 0.9945 - val_loss: 1.0997 - val_acc: 0.7918
Epoch 54/100
 - 70s - loss: 0.0210 - acc: 0.9944 - val_loss: 0.7799 - val_acc: 0.8316
Epoch 55/100
 - 70s - loss: 0.0210 - acc: 0.9949 - val_loss: 0.8885 - val_acc: 0.8200
Epoch 56/100
 - 70s - loss: 0.0202 - acc: 0.9945 - val_loss: 0.8319 - val_acc: 0.8344
Epoch 57/100
 - 70s - loss: 0.0182 - acc: 0.9952 - val_loss: 0.8791 - val_acc: 0.8216
Epoch 58/100
 - 70s - loss: 0.0187 - acc: 0.9950 - val_loss: 0.8123 - val_acc: 0.8344
Epoch 59/100
 - 70s - loss: 0.0183 - acc: 0.9950 - val_loss: 0.8808 - val_acc: 0.8204
Epoch 60/100
 - 70s - loss: 0.0183 - acc: 0.9955 - val_loss: 0.9019 - val_acc: 0.8222
Epoch 61/100
 - 70s - loss: 0.0167 - acc: 0.9959 - val_loss: 0.8713 - val_acc: 0.8306
Epoch 62/100
 - 70s - loss: 0.0152 - acc: 0.9963 - val_loss: 1.0702 - val_acc: 0.8018
Epoch 63/100
 - 70s - loss: 0.0144 - acc: 0.9966 - val_loss: 0.8917 - val_acc: 0.8254
Epoch 64/100
 - 70s - loss: 0.0152 - acc: 0.9960 - val_loss: 0.8546 - val_acc: 0.8288
Epoch 65/100
 - 70s - loss: 0.0149 - acc: 0.9961 - val_loss: 0.9477 - val_acc: 0.8238
Epoch 66/100
 - 70s - loss: 0.0149 - acc: 0.9963 - val_loss: 0.8573 - val_acc: 0.8296
Epoch 67/100
 - 70s - loss: 0.0145 - acc: 0.9964 - val_loss: 0.9067 - val_acc: 0.8272
Epoch 68/100
 - 70s - loss: 0.0133 - acc: 0.9971 - val_loss: 0.8541 - val_acc: 0.8318
Epoch 69/100
 - 70s - loss: 0.0113 - acc: 0.9974 - val_loss: 0.8177 - val_acc: 0.8422
Epoch 70/100
 - 70s - loss: 0.0124 - acc: 0.9970 - val_loss: 0.8851 - val_acc: 0.8300
Epoch 71/100
 - 70s - loss: 0.0125 - acc: 0.9966 - val_loss: 0.9090 - val_acc: 0.8256
Epoch 72/100
 - 70s - loss: 0.0115 - acc: 0.9971 - val_loss: 0.9035 - val_acc: 0.8298
Epoch 73/100
 - 70s - loss: 0.0105 - acc: 0.9977 - val_loss: 0.8003 - val_acc: 0.8422
Epoch 74/100
 - 70s - loss: 0.0108 - acc: 0.9976 - val_loss: 0.9749 - val_acc: 0.8154
Epoch 75/100
 - 70s - loss: 0.0105 - acc: 0.9975 - val_loss: 1.0341 - val_acc: 0.8142
Epoch 76/100
 - 70s - loss: 0.0099 - acc: 0.9978 - val_loss: 0.9660 - val_acc: 0.8174
Epoch 77/100
 - 70s - loss: 0.0102 - acc: 0.9980 - val_loss: 0.8576 - val_acc: 0.8364
Epoch 78/100
 - 70s - loss: 0.0107 - acc: 0.9972 - val_loss: 0.9938 - val_acc: 0.8174
Epoch 79/100
 - 70s - loss: 0.0094 - acc: 0.9980 - val_loss: 0.8806 - val_acc: 0.8334
Epoch 80/100
 - 70s - loss: 0.0092 - acc: 0.9980 - val_loss: 0.8977 - val_acc: 0.8282
Epoch 81/100
 - 70s - loss: 0.0090 - acc: 0.9979 - val_loss: 1.0563 - val_acc: 0.8182
Epoch 82/100
 - 70s - loss: 0.0092 - acc: 0.9980 - val_loss: 0.9429 - val_acc: 0.8288
Epoch 83/100
 - 70s - loss: 0.0090 - acc: 0.9980 - val_loss: 0.9241 - val_acc: 0.8292
Epoch 84/100
 - 70s - loss: 0.0089 - acc: 0.9980 - val_loss: 0.9735 - val_acc: 0.8198
Epoch 85/100
 - 70s - loss: 0.0084 - acc: 0.9984 - val_loss: 0.8365 - val_acc: 0.8410
Epoch 86/100
 - 70s - loss: 0.0089 - acc: 0.9980 - val_loss: 0.9191 - val_acc: 0.8276
Epoch 87/100
 - 70s - loss: 0.0085 - acc: 0.9980 - val_loss: 0.9655 - val_acc: 0.8260
Epoch 88/100
 - 70s - loss: 0.0084 - acc: 0.9982 - val_loss: 0.9269 - val_acc: 0.8282
Epoch 89/100
 - 70s - loss: 0.0077 - acc: 0.9985 - val_loss: 0.8851 - val_acc: 0.8340
Epoch 90/100
 - 70s - loss: 0.0083 - acc: 0.9981 - val_loss: 0.8930 - val_acc: 0.8340
Epoch 91/100
 - 70s - loss: 0.0085 - acc: 0.9981 - val_loss: 1.0093 - val_acc: 0.8156
Epoch 92/100
 - 70s - loss: 0.0083 - acc: 0.9978 - val_loss: 0.8646 - val_acc: 0.8370
Epoch 93/100
 - 70s - loss: 0.0074 - acc: 0.9983 - val_loss: 0.8674 - val_acc: 0.8386
Epoch 94/100
 - 70s - loss: 0.0070 - acc: 0.9987 - val_loss: 0.8831 - val_acc: 0.8388
Epoch 95/100
 - 70s - loss: 0.0069 - acc: 0.9986 - val_loss: 0.8600 - val_acc: 0.8398
Epoch 96/100
 - 70s - loss: 0.0069 - acc: 0.9986 - val_loss: 0.9267 - val_acc: 0.8294
Epoch 97/100
 - 70s - loss: 0.0072 - acc: 0.9983 - val_loss: 0.8985 - val_acc: 0.8374
Epoch 98/100
 - 70s - loss: 0.0067 - acc: 0.9986 - val_loss: 0.9141 - val_acc: 0.8306
Epoch 99/100
 - 70s - loss: 0.0066 - acc: 0.9985 - val_loss: 0.9324 - val_acc: 0.8286
Epoch 100/100
 - 70s - loss: 0.0069 - acc: 0.9986 - val_loss: 0.8871 - val_acc: 0.8390
6992.177407026291
--------
{'val_loss': [1.8972355087280273, 1.9551137680053712, 1.4322291299819947, 1.174679866027832, 1.2311778377532958, 1.1032071083068848, 0.9882226833343506, 0.8786930030822754, 1.5214132362365722, 0.9172868747711181, 0.8768379720687867, 0.8562384883880615, 0.7635606281280518, 0.700561870098114, 1.1006230499267577, 0.7319197127342224, 0.7892989656448365, 0.663444792175293, 0.8325383823394775, 0.7565259029388428, 0.6733176401138306, 1.0476155792236328, 0.7597126739501953, 0.9659814180374146, 0.9001781450271606, 0.6874009233474732, 0.8968337275505066, 0.9310425483703614, 0.7378865934371949, 0.7961516748428344, 0.7184248123168945, 0.856350182723999, 0.8013051399230957, 1.3848864364624023, 0.9844520236968994, 0.7659518466949463, 0.776811909866333, 1.1191267902374267, 0.9585070518493652, 0.8631832775115966, 0.865363923072815, 0.951260848903656, 0.8748479686737061, 1.0278778003692628, 0.8323201419830323, 0.9954553279876709, 1.1331724998474122, 0.7381479122161865, 1.1901365892410278, 1.0200588790893554, 1.0270220987319947, 0.8488869911193848, 1.0997381401062012, 0.7799414081573486, 0.8884655601501464, 0.8319078974723816, 0.8791264789581299, 0.8122988117218017, 0.8807613065719605, 0.901857473373413, 0.8712670505523682, 1.070217055130005, 0.8916694213867188, 0.8545618072509765, 0.9476796966552734, 0.8572636093139648, 0.9067339832305908, 0.8541263423919677, 0.8177495079040528, 0.8850810146331787, 0.9089698089599609, 0.9034608966827392, 0.8003257892608643, 0.9749427482604981, 1.034138243484497, 0.9659621208190918, 0.8576289735794067, 0.9937581794738769, 0.8806126991271973, 0.8977006267547607, 1.0562529403686522, 0.9428971393585205, 0.9240832721710205, 0.9734738716125488, 0.8364508068084717, 0.9190980951309204, 0.9654503221511841, 0.9269451541900635, 0.8851091506958008, 0.8929567935943603, 1.009326176071167, 0.8645646194458008, 0.8673509902954102, 0.8831410911560058, 0.8599711944580078, 0.9267188262939453, 0.8985324314117432, 0.9140946224212646, 0.9324068864822388, 0.8871010982513428], 'val_acc': [0.3632, 0.3542, 0.515, 0.5848, 0.5898, 0.6326, 0.6588, 0.7018, 0.5578, 0.6854, 0.7112, 0.719, 0.7588, 0.768, 0.6626, 0.7728, 0.7462, 0.7922, 0.752, 0.7818, 0.7902, 0.7334, 0.7738, 0.747, 0.7574, 0.807, 0.775, 0.7716, 0.8134, 0.797, 0.8194, 0.7864, 0.8012, 0.716, 0.7774, 0.8234, 0.816, 0.7774, 0.7988, 0.812, 0.8124, 0.8054, 0.8172, 0.7878, 0.8208, 0.7998, 0.7856, 0.8384, 0.7874, 0.7962, 0.8062, 0.8212, 0.7918, 0.8316, 0.82, 0.8344, 0.8216, 0.8344, 0.8204, 0.8222, 0.8306, 0.8018, 0.8254, 0.8288, 0.8238, 0.8296, 0.8272, 0.8318, 0.8422, 0.83, 0.8256, 0.8298, 0.8422, 0.8154, 0.8142, 0.8174, 0.8364, 0.8174, 0.8334, 0.8282, 0.8182, 0.8288, 0.8292, 0.8198, 0.841, 0.8276, 0.826, 0.8282, 0.834, 0.834, 0.8156, 0.837, 0.8386, 0.8388, 0.8398, 0.8294, 0.8374, 0.8306, 0.8286, 0.839], 'loss': [3.988615212504069, 1.546594533390469, 1.3162968973795572, 1.1483594299316406, 0.9858493003845215, 0.8725718406253391, 0.7897347419208951, 0.7189122085571289, 0.6588371070755853, 0.6084237012174395, 0.562212206586202, 0.5200131482336257, 0.4850391995747884, 0.44578337019814385, 0.4111532806237539, 0.37798321579297384, 0.35573546598752337, 0.33074537541601395, 0.30498925534354315, 0.2790583981566959, 0.2544560490873125, 0.2364981001271142, 0.2170012355354097, 0.19639372716214923, 0.17979371367295582, 0.16352947605715856, 0.15045364371935527, 0.1363260450522105, 0.12425906239880456, 0.10895277855793635, 0.09923719279898538, 0.09458744992282656, 0.08448702081839243, 0.07649953385194143, 0.07077237713403171, 0.061998585022820366, 0.05826619076795048, 0.05288489677442445, 0.04938704029321671, 0.04965491589042875, 0.04420322313308716, 0.04068209378388193, 0.0401188702583313, 0.036444369048542445, 0.03393106140957938, 0.03321153269873725, 0.031428508874442845, 0.029584499357475175, 0.027979100235303242, 0.02704094854593277, 0.0250969023598565, 0.024859809119171565, 0.02219397296110789, 0.02096164585881763, 0.020981526600321133, 0.020229039764404298, 0.018248209606607755, 0.018749969835910534, 0.018266409823298455, 0.018336538117792872, 0.016697360175972183, 0.015190459425581827, 0.014447713634702894, 0.015173642103539572, 0.014871677453650369, 0.014940526764757104, 0.014521758387734493, 0.013297018546528287, 0.011307000880440076, 0.012353085392465194, 0.012478454514841239, 0.011504892194271088, 0.010519022605816523, 0.010828751041491827, 0.010452545193168851, 0.009855043699343998, 0.010203786462876532, 0.010683689420918624, 0.009369981665329801, 0.009170196849852801, 0.009012362095713615, 0.009188489624195629, 0.009012245304882526, 0.008943082714122202, 0.008449449289507336, 0.008899634671211243, 0.008512243051661386, 0.008369031490882238, 0.007652563188183639, 0.008282492162618373, 0.008482897056846156, 0.008293604398270447, 0.007391181967655818, 0.006967684372266134, 0.0069266147730251155, 0.006892491028706232, 0.007198080322063632, 0.006725794449283017, 0.0066191147425108485, 0.006856206335624059], 'acc': [0.2994222222222222, 0.47222222220102944, 0.5455333333121406, 0.5996222222328186, 0.6488222222010295, 0.6911333333015441, 0.7204222222540113, 0.7459111111323039, 0.7662444444656372, 0.7854000000211928, 0.8025555555237665, 0.8176444444762336, 0.8303777777883742, 0.8442888888994853, 0.8577111110687256, 0.8691111110899183, 0.8740222222540114, 0.8840666666560703, 0.8940888889100816, 0.9029111111534966, 0.9133333333651225, 0.9182000000317891, 0.9250666666878594, 0.9326888888465034, 0.9392444444550408, 0.946111111079322, 0.9484000000211927, 0.9542222221904331, 0.9593555555343628, 0.9646666666560703, 0.9674888888676961, 0.968511111079322, 0.9734888888676961, 0.9760888888570998, 0.9778888888782925, 0.9812666666560703, 0.9815111111005147, 0.9838666666666667, 0.9851777777671814, 0.9853999999894036, 0.9870666666560702, 0.9886666666560703, 0.9881111111111112, 0.9896666666348776, 0.9900666666560702, 0.9900888889100816, 0.9908888888782925, 0.9919333333227369, 0.9922888888676962, 0.9926666666560703, 0.9936666666454739, 0.9926222221904331, 0.994533333322737, 0.994444444433848, 0.9949111111111111, 0.9944666666560703, 0.9952222222116258, 0.9950222222222223, 0.995, 0.9954666666560703, 0.9958666666666667, 0.9963333333333333, 0.9966222222222222, 0.9959777777777777, 0.9960666666666667, 0.9962888888888889, 0.9964444444444445, 0.9970888888782925, 0.9974444444232516, 0.9969555555555556, 0.9966444444444444, 0.9970888888888889, 0.9977333333333334, 0.9975555555449592, 0.9974888888782925, 0.9978222222116259, 0.9979777777777777, 0.9972444444444445, 0.9980444444444444, 0.9979555555555556, 0.9979111111005148, 0.9979777777671814, 0.9980444444444444, 0.9979555555555556, 0.9984444444444445, 0.9980222222116258, 0.9980444444444444, 0.9982444444444445, 0.9985333333333334, 0.9981111111111111, 0.9980666666666667, 0.9978222222222223, 0.9983333333333333, 0.9986888888782925, 0.9986444444444444, 0.9985555555555555, 0.9983111111111111, 0.9985555555555555, 0.9984888888782925, 0.9986]}
===Final Test Score===
Test loss: 0.8655857063174248
Test accuracy: 0.8374
