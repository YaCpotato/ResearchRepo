[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 16773008064739172162
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 5297098034323938529
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:1"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 10797543866783192728
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:2"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 4897458687426002871
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:3"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 13331787634846979024
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 11254190715911873093
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 17457053497358289510
physical_device_desc: "device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3d:00.0, compute capability: 7.0"
, name: "/device:GPU:1"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 12973701860755894819
physical_device_desc: "device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3e:00.0, compute capability: 7.0"
, name: "/device:GPU:2"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 13562032797923044047
physical_device_desc: "device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b1:00.0, compute capability: 7.0"
, name: "/device:GPU:3"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 13665535272562983155
physical_device_desc: "device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b2:00.0, compute capability: 7.0"
]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 160)  0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 160)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 160)  0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 160)  0           activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16, 16, 320)  0           activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 16, 16, 320)  0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      dropout_6[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 16, 320)  0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      dropout_7[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 16, 16, 320)  0           activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 8, 8, 640)    0           activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 8, 8, 640)    0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 8, 8, 640)    0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 8, 8, 640)    0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 100)          64100       global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,564,932
Trainable params: 36,546,980
Non-trainable params: 17,952
__________________________________________________________________________________________________
Train on 45000 samples, validate on 5000 samples
Epoch 1/100
 - 79s - loss: 4.4844 - acc: 0.0879 - val_loss: 4.7562 - val_acc: 0.0480
Epoch 2/100
 - 70s - loss: 3.5261 - acc: 0.1818 - val_loss: 3.7854 - val_acc: 0.1486
Epoch 3/100
 - 70s - loss: 3.1189 - acc: 0.2589 - val_loss: 4.0237 - val_acc: 0.1632
Epoch 4/100
 - 70s - loss: 2.8331 - acc: 0.3157 - val_loss: 3.5835 - val_acc: 0.2082
Epoch 5/100
 - 70s - loss: 2.6168 - acc: 0.3619 - val_loss: 3.2233 - val_acc: 0.2752
Epoch 6/100
 - 70s - loss: 2.3401 - acc: 0.3982 - val_loss: 3.1240 - val_acc: 0.2712
Epoch 7/100
 - 70s - loss: 2.1646 - acc: 0.4336 - val_loss: 3.0409 - val_acc: 0.2848
Epoch 8/100
 - 70s - loss: 2.0239 - acc: 0.4639 - val_loss: 2.5571 - val_acc: 0.3626
Epoch 9/100
 - 70s - loss: 1.9089 - acc: 0.4934 - val_loss: 3.8794 - val_acc: 0.2402
Epoch 10/100
 - 70s - loss: 1.8064 - acc: 0.5176 - val_loss: 2.6356 - val_acc: 0.3548
Epoch 11/100
 - 70s - loss: 1.7075 - acc: 0.5418 - val_loss: 2.3965 - val_acc: 0.3978
Epoch 12/100
 - 70s - loss: 1.6242 - acc: 0.5608 - val_loss: 2.4570 - val_acc: 0.3960
Epoch 13/100
 - 70s - loss: 1.5460 - acc: 0.5785 - val_loss: 2.5456 - val_acc: 0.3724
Epoch 14/100
 - 70s - loss: 1.4711 - acc: 0.5993 - val_loss: 3.0201 - val_acc: 0.3354
Epoch 15/100
 - 70s - loss: 1.4057 - acc: 0.6164 - val_loss: 2.3383 - val_acc: 0.4258
Epoch 16/100
 - 70s - loss: 1.3403 - acc: 0.6320 - val_loss: 2.4800 - val_acc: 0.3968
Epoch 17/100
 - 70s - loss: 1.2789 - acc: 0.6475 - val_loss: 2.3960 - val_acc: 0.4020
Epoch 18/100
 - 70s - loss: 1.2232 - acc: 0.6611 - val_loss: 2.4776 - val_acc: 0.3998
Epoch 19/100
 - 70s - loss: 1.1663 - acc: 0.6768 - val_loss: 2.4800 - val_acc: 0.4034
Epoch 20/100
 - 70s - loss: 1.1149 - acc: 0.6888 - val_loss: 2.3946 - val_acc: 0.4238
Epoch 21/100
 - 70s - loss: 1.0663 - acc: 0.7023 - val_loss: 2.5271 - val_acc: 0.4094
Epoch 22/100
 - 70s - loss: 1.0188 - acc: 0.7159 - val_loss: 2.4999 - val_acc: 0.4330
Epoch 23/100
 - 70s - loss: 0.9679 - acc: 0.7318 - val_loss: 2.1858 - val_acc: 0.4592
Epoch 24/100
 - 70s - loss: 0.9231 - acc: 0.7420 - val_loss: 2.2455 - val_acc: 0.4638
Epoch 25/100
 - 70s - loss: 0.8838 - acc: 0.7541 - val_loss: 2.2291 - val_acc: 0.4658
Epoch 26/100
 - 70s - loss: 0.8435 - acc: 0.7650 - val_loss: 2.5349 - val_acc: 0.4206
Epoch 27/100
 - 70s - loss: 0.8031 - acc: 0.7772 - val_loss: 2.2341 - val_acc: 0.4634
Epoch 28/100
 - 70s - loss: 0.7614 - acc: 0.7908 - val_loss: 2.5891 - val_acc: 0.4242
Epoch 29/100
 - 70s - loss: 0.7196 - acc: 0.8031 - val_loss: 2.6771 - val_acc: 0.4146
Epoch 30/100
 - 70s - loss: 0.6865 - acc: 0.8090 - val_loss: 2.1010 - val_acc: 0.5022
Epoch 31/100
 - 70s - loss: 0.6495 - acc: 0.8238 - val_loss: 2.4726 - val_acc: 0.4520
Epoch 32/100
 - 70s - loss: 0.6197 - acc: 0.8310 - val_loss: 2.1429 - val_acc: 0.4906
Epoch 33/100
 - 70s - loss: 0.5831 - acc: 0.8442 - val_loss: 2.3725 - val_acc: 0.4564
Epoch 34/100
 - 70s - loss: 0.5513 - acc: 0.8531 - val_loss: 2.2515 - val_acc: 0.4816
Epoch 35/100
 - 70s - loss: 0.5266 - acc: 0.8603 - val_loss: 2.8727 - val_acc: 0.4072
Epoch 36/100
 - 70s - loss: 0.4913 - acc: 0.8713 - val_loss: 2.0599 - val_acc: 0.5120
Epoch 37/100
 - 70s - loss: 0.4666 - acc: 0.8786 - val_loss: 2.2549 - val_acc: 0.4744
Epoch 38/100
 - 70s - loss: 0.4389 - acc: 0.8876 - val_loss: 2.6714 - val_acc: 0.4434
Epoch 39/100
 - 70s - loss: 0.4142 - acc: 0.8939 - val_loss: 2.1999 - val_acc: 0.5000
Epoch 40/100
 - 70s - loss: 0.3912 - acc: 0.9037 - val_loss: 2.3320 - val_acc: 0.4832
Epoch 41/100
 - 70s - loss: 0.3676 - acc: 0.9096 - val_loss: 2.2683 - val_acc: 0.5008
Epoch 42/100
 - 70s - loss: 0.3412 - acc: 0.9204 - val_loss: 2.0868 - val_acc: 0.5150
Epoch 43/100
 - 70s - loss: 0.3249 - acc: 0.9233 - val_loss: 2.2206 - val_acc: 0.4996
Epoch 44/100
 - 70s - loss: 0.3049 - acc: 0.9299 - val_loss: 2.2710 - val_acc: 0.5048
Epoch 45/100
 - 70s - loss: 0.2876 - acc: 0.9350 - val_loss: 2.8579 - val_acc: 0.4280
Epoch 46/100
 - 70s - loss: 0.2725 - acc: 0.9401 - val_loss: 2.3969 - val_acc: 0.4934
Epoch 47/100
 - 70s - loss: 0.2564 - acc: 0.9438 - val_loss: 2.2804 - val_acc: 0.4998
Epoch 48/100
 - 70s - loss: 0.2371 - acc: 0.9512 - val_loss: 2.1992 - val_acc: 0.5190
Epoch 49/100
 - 70s - loss: 0.2265 - acc: 0.9534 - val_loss: 2.2398 - val_acc: 0.5078
Epoch 50/100
 - 70s - loss: 0.2134 - acc: 0.9575 - val_loss: 2.3629 - val_acc: 0.4900
Epoch 51/100
 - 70s - loss: 0.2006 - acc: 0.9616 - val_loss: 2.2643 - val_acc: 0.5138
Epoch 52/100
 - 70s - loss: 0.1894 - acc: 0.9661 - val_loss: 2.2815 - val_acc: 0.5146
Epoch 53/100
 - 70s - loss: 0.1800 - acc: 0.9670 - val_loss: 2.3336 - val_acc: 0.4994
Epoch 54/100
 - 70s - loss: 0.1741 - acc: 0.9698 - val_loss: 2.0710 - val_acc: 0.5388
Epoch 55/100
 - 70s - loss: 0.1625 - acc: 0.9724 - val_loss: 2.4202 - val_acc: 0.5004
Epoch 56/100
 - 70s - loss: 0.1552 - acc: 0.9742 - val_loss: 2.3395 - val_acc: 0.5108
Epoch 57/100
 - 70s - loss: 0.1458 - acc: 0.9766 - val_loss: 2.4237 - val_acc: 0.5014
Epoch 58/100
 - 70s - loss: 0.1394 - acc: 0.9785 - val_loss: 2.3095 - val_acc: 0.5226
Epoch 59/100
 - 70s - loss: 0.1326 - acc: 0.9798 - val_loss: 2.4145 - val_acc: 0.5056
Epoch 60/100
 - 70s - loss: 0.1268 - acc: 0.9812 - val_loss: 2.2307 - val_acc: 0.5202
Epoch 61/100
 - 70s - loss: 0.1190 - acc: 0.9830 - val_loss: 2.3767 - val_acc: 0.5202
Epoch 62/100
 - 70s - loss: 0.1144 - acc: 0.9848 - val_loss: 2.3611 - val_acc: 0.5138
Epoch 63/100
 - 70s - loss: 0.1085 - acc: 0.9861 - val_loss: 2.4081 - val_acc: 0.5154
Epoch 64/100
 - 70s - loss: 0.1041 - acc: 0.9869 - val_loss: 2.3409 - val_acc: 0.5216
Epoch 65/100
 - 70s - loss: 0.1006 - acc: 0.9882 - val_loss: 2.3666 - val_acc: 0.5200
Epoch 66/100
 - 70s - loss: 0.0965 - acc: 0.9880 - val_loss: 2.2635 - val_acc: 0.5290
Epoch 67/100
 - 70s - loss: 0.0911 - acc: 0.9896 - val_loss: 2.3146 - val_acc: 0.5254
Epoch 68/100
 - 70s - loss: 0.0894 - acc: 0.9895 - val_loss: 2.3281 - val_acc: 0.5208
Epoch 69/100
 - 70s - loss: 0.0867 - acc: 0.9900 - val_loss: 2.3046 - val_acc: 0.5372
Epoch 70/100
 - 70s - loss: 0.0823 - acc: 0.9911 - val_loss: 2.3271 - val_acc: 0.5272
Epoch 71/100
 - 70s - loss: 0.0802 - acc: 0.9910 - val_loss: 2.2829 - val_acc: 0.5354
Epoch 72/100
 - 70s - loss: 0.0776 - acc: 0.9923 - val_loss: 2.4752 - val_acc: 0.5082
Epoch 73/100
 - 70s - loss: 0.0756 - acc: 0.9922 - val_loss: 2.2859 - val_acc: 0.5416
Epoch 74/100
 - 70s - loss: 0.0726 - acc: 0.9927 - val_loss: 2.3662 - val_acc: 0.5280
Epoch 75/100
 - 70s - loss: 0.0705 - acc: 0.9925 - val_loss: 2.2450 - val_acc: 0.5450
Epoch 76/100
 - 70s - loss: 0.0670 - acc: 0.9935 - val_loss: 2.3378 - val_acc: 0.5320
Epoch 77/100
 - 70s - loss: 0.0655 - acc: 0.9939 - val_loss: 2.3357 - val_acc: 0.5356
Epoch 78/100
 - 70s - loss: 0.0640 - acc: 0.9944 - val_loss: 2.3937 - val_acc: 0.5286
Epoch 79/100
 - 70s - loss: 0.0619 - acc: 0.9943 - val_loss: 2.3118 - val_acc: 0.5374
Epoch 80/100
 - 70s - loss: 0.0597 - acc: 0.9949 - val_loss: 2.3398 - val_acc: 0.5370
Epoch 81/100
 - 70s - loss: 0.0577 - acc: 0.9951 - val_loss: 2.3175 - val_acc: 0.5408
Epoch 82/100
 - 70s - loss: 0.0564 - acc: 0.9951 - val_loss: 2.4303 - val_acc: 0.5308
Epoch 83/100
 - 70s - loss: 0.0557 - acc: 0.9950 - val_loss: 2.3675 - val_acc: 0.5306
Epoch 84/100
 - 70s - loss: 0.0555 - acc: 0.9946 - val_loss: 2.3793 - val_acc: 0.5354
Epoch 85/100
 - 70s - loss: 0.0524 - acc: 0.9954 - val_loss: 2.4307 - val_acc: 0.5272
Epoch 86/100
 - 70s - loss: 0.0509 - acc: 0.9957 - val_loss: 2.3436 - val_acc: 0.5420
Epoch 87/100
 - 70s - loss: 0.0505 - acc: 0.9952 - val_loss: 2.4506 - val_acc: 0.5240
Epoch 88/100
 - 70s - loss: 0.0493 - acc: 0.9959 - val_loss: 2.3434 - val_acc: 0.5402
Epoch 89/100
 - 70s - loss: 0.0489 - acc: 0.9960 - val_loss: 2.2665 - val_acc: 0.5510
Epoch 90/100
 - 70s - loss: 0.0452 - acc: 0.9966 - val_loss: 2.3227 - val_acc: 0.5456
Epoch 91/100
 - 70s - loss: 0.0446 - acc: 0.9971 - val_loss: 2.3717 - val_acc: 0.5408
Epoch 92/100
 - 70s - loss: 0.0443 - acc: 0.9969 - val_loss: 2.3144 - val_acc: 0.5438
Epoch 93/100
 - 70s - loss: 0.0433 - acc: 0.9971 - val_loss: 2.3684 - val_acc: 0.5396
Epoch 94/100
 - 70s - loss: 0.0432 - acc: 0.9969 - val_loss: 2.3021 - val_acc: 0.5420
Epoch 95/100
 - 70s - loss: 0.0420 - acc: 0.9973 - val_loss: 2.3553 - val_acc: 0.5424
Epoch 96/100
 - 70s - loss: 0.0423 - acc: 0.9969 - val_loss: 2.3692 - val_acc: 0.5398
Epoch 97/100
 - 70s - loss: 0.0393 - acc: 0.9974 - val_loss: 2.3688 - val_acc: 0.5402
Epoch 98/100
 - 70s - loss: 0.0401 - acc: 0.9975 - val_loss: 2.3531 - val_acc: 0.5422
Epoch 99/100
 - 70s - loss: 0.0398 - acc: 0.9969 - val_loss: 2.3933 - val_acc: 0.5382
Epoch 100/100
 - 70s - loss: 0.0384 - acc: 0.9973 - val_loss: 2.4393 - val_acc: 0.5388
7017.7416660785675
--------
{'val_loss': [4.756231969451904, 3.7853638103485108, 4.023665884399414, 3.583523459625244, 3.2233078353881837, 3.124017406463623, 3.0408958339691163, 2.55708059387207, 3.8794356002807615, 2.635613645172119, 2.396521381187439, 2.4569631996154784, 2.54558825378418, 3.02010959854126, 2.3382547248840333, 2.4800324331283568, 2.3960191272735596, 2.4776304443359374, 2.4800277034759524, 2.3946323768615723, 2.5271040077209475, 2.4999428665161134, 2.1858177913665773, 2.2455398498535155, 2.2290596157073974, 2.534913706970215, 2.2340900384902955, 2.58906287689209, 2.677080001068115, 2.1010015075683595, 2.47258362159729, 2.142898978805542, 2.3724617752075194, 2.2515485773086548, 2.8727082611083983, 2.059906103515625, 2.254868346405029, 2.671373221206665, 2.1998843019485474, 2.3320326160430906, 2.2682652938842773, 2.0867637702941892, 2.220611787414551, 2.271028371810913, 2.857902938079834, 2.396885888671875, 2.2804468505859377, 2.1991580009460447, 2.239813472366333, 2.3628831077575683, 2.264301210021973, 2.2815309257507326, 2.3335975093841554, 2.070968920135498, 2.420212910079956, 2.3394764488220217, 2.4236935386657716, 2.309520845794678, 2.414528072357178, 2.2307434371948243, 2.3767172649383546, 2.3610506721496582, 2.4081319660186766, 2.340885287475586, 2.3665739574432374, 2.2635294692993164, 2.3146458877563476, 2.328094506072998, 2.3046151737213134, 2.327140510177612, 2.2829423988342286, 2.4751583614349366, 2.2859231666564943, 2.366167460632324, 2.245029476928711, 2.337781533050537, 2.3357082454681395, 2.3936518112182616, 2.3117783252716064, 2.339773991394043, 2.3174523147583006, 2.4302942001342775, 2.367506636810303, 2.3793338287353514, 2.4306996509552, 2.3435525634765626, 2.450553729248047, 2.343416440963745, 2.2664897289276125, 2.322675355911255, 2.37165122795105, 2.31440498085022, 2.3683736354827882, 2.3021384284973143, 2.3552779819488525, 2.3692432525634763, 2.3687772827148437, 2.3530952598571777, 2.3932678310394286, 2.4393021827697754], 'val_acc': [0.048, 0.1486, 0.1632, 0.2082, 0.2752, 0.2712, 0.2848, 0.3626, 0.2402, 0.3548, 0.3978, 0.396, 0.3724, 0.3354, 0.4258, 0.3968, 0.402, 0.3998, 0.4034, 0.4238, 0.4094, 0.433, 0.4592, 0.4638, 0.4658, 0.4206, 0.4634, 0.4242, 0.4146, 0.5022, 0.452, 0.4906, 0.4564, 0.4816, 0.4072, 0.512, 0.4744, 0.4434, 0.5, 0.4832, 0.5008, 0.515, 0.4996, 0.5048, 0.428, 0.4934, 0.4998, 0.519, 0.5078, 0.49, 0.5138, 0.5146, 0.4994, 0.5388, 0.5004, 0.5108, 0.5014, 0.5226, 0.5056, 0.5202, 0.5202, 0.5138, 0.5154, 0.5216, 0.52, 0.529, 0.5254, 0.5208, 0.5372, 0.5272, 0.5354, 0.5082, 0.5416, 0.528, 0.545, 0.532, 0.5356, 0.5286, 0.5374, 0.537, 0.5408, 0.5308, 0.5306, 0.5354, 0.5272, 0.542, 0.524, 0.5402, 0.551, 0.5456, 0.5408, 0.5438, 0.5396, 0.542, 0.5424, 0.5398, 0.5402, 0.5422, 0.5382, 0.5388], 'loss': [4.4844367674085825, 3.526103026707967, 3.118867755932278, 2.8331393279181585, 2.616750187471178, 2.3401182779100207, 2.1646046657138402, 2.0239086026509603, 1.9088784379111396, 1.8064326226764256, 1.7075202912224663, 1.6241755946901109, 1.5460265825271606, 1.4711121143976846, 1.4057052914301555, 1.3402603054894342, 1.2789377286699084, 1.2231637153625488, 1.1663089620166354, 1.1148877392027112, 1.066345190027025, 1.018828393152025, 0.9678643153296577, 0.923052221997579, 0.8837727121882969, 0.8434951463487413, 0.8030691775533888, 0.7613623359468248, 0.7195679146448771, 0.68649725634257, 0.64954859951867, 0.6197350975990296, 0.5830661659240722, 0.5512792217042711, 0.526559597131941, 0.49129089387257896, 0.4665660834100511, 0.43886338347858855, 0.41419437783029345, 0.39116938279999625, 0.3675620849503411, 0.3412477957248688, 0.3249156602011787, 0.30488266531626385, 0.28764421255323624, 0.27254938742319745, 0.2564202244440715, 0.23707061115900677, 0.2264553574959437, 0.21338862900204128, 0.200570922115114, 0.18939487850401138, 0.1800337044292026, 0.17413367598851523, 0.16245883904827965, 0.15517297305001154, 0.14575624284214445, 0.13941483636697133, 0.13260273770226372, 0.12682347479926215, 0.1190268850988812, 0.11437904470496707, 0.10852506068282657, 0.10405705630779266, 0.10055708763599396, 0.09645336911943224, 0.09105668006473118, 0.08939808696640862, 0.08674998125765059, 0.08227567735513051, 0.08022361450592677, 0.0776173455370797, 0.07560988935099708, 0.07255988376935323, 0.07052169156869252, 0.06695375689533022, 0.06553725169301033, 0.06395856603781382, 0.061906592643260955, 0.05965701411962509, 0.057711109387874604, 0.056375484380457135, 0.05567276412314839, 0.0554539329316881, 0.05244508621030384, 0.05089191174507141, 0.05048195261822806, 0.04925676183303197, 0.048869627613491486, 0.045177038187450835, 0.04461951109899415, 0.04431949316594336, 0.04330400343139967, 0.04318033587800132, 0.04197766702572504, 0.04226588654120763, 0.03929512924551964, 0.04006516318056318, 0.03976834173003833, 0.03841946480406655], 'acc': [0.08793333334392972, 0.18175555555290646, 0.25886666667726305, 0.31566666666666665, 0.3618888888782925, 0.39817777778307595, 0.4336222222063276, 0.4638666666825612, 0.4934, 0.5175777777883741, 0.5418444444232516, 0.5608444444444445, 0.5784888888994852, 0.5993111111005147, 0.616355555566152, 0.6319999999788073, 0.6474666666560703, 0.6610888889312744, 0.6768222222540113, 0.68875555551317, 0.7023333333545261, 0.7159333333651224, 0.7317999999788073, 0.7419999999788073, 0.7541111111534966, 0.7650222222222223, 0.7772222222540114, 0.7908444444020589, 0.8030666667090522, 0.809022222243415, 0.8237999999788073, 0.8309999999788072, 0.8441777777353923, 0.8530888888994853, 0.8603333333545261, 0.8713333333333333, 0.8785999999788072, 0.8875999999682108, 0.8939333333439297, 0.9037333333757188, 0.9096444444444445, 0.9203777778201633, 0.9233333333757189, 0.929933333322737, 0.93504444448683, 0.9400666666984558, 0.943755555566152, 0.9512444444020589, 0.9534444444232517, 0.9574888889100817, 0.9616444444126553, 0.966111111079322, 0.9670222221904331, 0.969777777809567, 0.9724222222116259, 0.9741555555449591, 0.9766222222116259, 0.9784666666666667, 0.9798222222222223, 0.981177777756585, 0.9830444444338481, 0.9847999999576145, 0.9861333332909478, 0.9869111110899184, 0.98815555551317, 0.9879777777459886, 0.9895999999682109, 0.9895333333333334, 0.9899999999682109, 0.9910666666666667, 0.991, 0.992311111079322, 0.9921999999894037, 0.9927111111111111, 0.9924666666560703, 0.993533333322737, 0.9938666666666667, 0.994377777756585, 0.9942666666666666, 0.9949111111005148, 0.9951333333333333, 0.9950888888782925, 0.9950222222116258, 0.994644444433848, 0.9953555555343628, 0.9956666666666667, 0.9952444444444445, 0.9959111111111111, 0.9960444444338481, 0.9966444444232517, 0.9970888888782925, 0.9969111111005148, 0.9971333333333333, 0.9969111111005148, 0.9972888888888889, 0.9968666666560703, 0.9973555555555556, 0.9975111111111111, 0.9968888888782925, 0.9973333333333333]}
===Final Test Score===
Test loss: 2.4455649139404296
Test accuracy: 0.5447
