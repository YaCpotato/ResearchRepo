wrn_28_10 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 100)          64100       global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,564,932
Trainable params: 36,546,980
Non-trainable params: 17,952
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'additive-gaussian-noise', 'aug1_magnitude': 0.795, 'aug2_type': 'brighten', 'aug2_magnitude': 0.7120000000000001}, {'aug1_type': 'brighten', 'aug1_magnitude': 0.003, 'aug2_type': 'shear', 'aug2_magnitude': 0.012}, {'aug1_type': 'vertical-flip', 'aug1_magnitude': 0.7390000000000001, 'aug2_type': 'translate-x', 'aug2_magnitude': 0.322}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.20800000000000002, 'aug2_type': 'coarse-dropout', 'aug2_magnitude': 0.475}, {'aug1_type': 'translate-y', 'aug1_magnitude': 0.7809999999999999, 'aug2_type': 'coarse-dropout', 'aug2_magnitude': 0.789}]

 - 100s - loss: 4.5220 - acc: 0.0598 - val_loss: 4.5745 - val_acc: 0.0792
Epoch 2/100
 - 88s - loss: 3.9882 - acc: 0.1078 - val_loss: 3.7447 - val_acc: 0.1588
Epoch 3/100
 - 88s - loss: 3.6549 - acc: 0.1602 - val_loss: 3.5305 - val_acc: 0.2045
Epoch 4/100
 - 88s - loss: 3.3359 - acc: 0.2170 - val_loss: 2.9386 - val_acc: 0.2993
Epoch 5/100
 - 88s - loss: 3.0907 - acc: 0.2611 - val_loss: 3.1427 - val_acc: 0.3223
Epoch 6/100
 - 90s - loss: 2.8657 - acc: 0.3031 - val_loss: 2.5546 - val_acc: 0.3937
Epoch 7/100
 - 89s - loss: 2.6815 - acc: 0.3423 - val_loss: 2.5250 - val_acc: 0.3938
Epoch 8/100
 - 89s - loss: 2.5291 - acc: 0.3735 - val_loss: 2.1696 - val_acc: 0.4368
Epoch 9/100
 - 90s - loss: 2.3689 - acc: 0.4045 - val_loss: 2.0535 - val_acc: 0.4677
Epoch 10/100
 - 89s - loss: 2.2753 - acc: 0.4309 - val_loss: 1.8959 - val_acc: 0.5038
Epoch 11/100
 - 90s - loss: 2.1578 - acc: 0.4543 - val_loss: 2.0292 - val_acc: 0.4732
Epoch 12/100
 - 89s - loss: 2.0627 - acc: 0.4763 - val_loss: 1.9719 - val_acc: 0.5230
Epoch 13/100
 - 90s - loss: 1.9750 - acc: 0.5002 - val_loss: 1.6709 - val_acc: 0.5610
Epoch 14/100
 - 89s - loss: 1.8983 - acc: 0.5154 - val_loss: 1.7376 - val_acc: 0.5488
Epoch 15/100
 - 89s - loss: 1.8189 - acc: 0.5348 - val_loss: 1.7607 - val_acc: 0.5548
Epoch 16/100
 - 89s - loss: 1.7029 - acc: 0.5520 - val_loss: 1.7322 - val_acc: 0.5812
Epoch 17/100
 - 89s - loss: 1.6264 - acc: 0.5660 - val_loss: 1.4844 - val_acc: 0.5975
Epoch 18/100
 - 89s - loss: 1.5358 - acc: 0.5862 - val_loss: 1.6397 - val_acc: 0.5842
Epoch 19/100
 - 89s - loss: 1.4927 - acc: 0.5984 - val_loss: 1.6138 - val_acc: 0.6012
Epoch 20/100
 - 89s - loss: 1.4137 - acc: 0.6147 - val_loss: 1.4353 - val_acc: 0.6248
Epoch 21/100
 - 89s - loss: 1.3665 - acc: 0.6286 - val_loss: 1.4943 - val_acc: 0.6253
Epoch 22/100
 - 89s - loss: 1.3119 - acc: 0.6420 - val_loss: 1.6913 - val_acc: 0.5957
Epoch 23/100
 - 89s - loss: 1.2705 - acc: 0.6529 - val_loss: 1.5348 - val_acc: 0.6320
Epoch 24/100
 - 89s - loss: 1.2242 - acc: 0.6649 - val_loss: 1.4129 - val_acc: 0.6398
Epoch 25/100
 - 88s - loss: 1.1742 - acc: 0.6777 - val_loss: 1.5766 - val_acc: 0.6427
Epoch 26/100
 - 88s - loss: 1.1212 - acc: 0.6932 - val_loss: 1.3941 - val_acc: 0.6552
Epoch 27/100
 - 88s - loss: 1.1007 - acc: 0.7016 - val_loss: 1.5711 - val_acc: 0.6362
Epoch 28/100
 - 86s - loss: 1.0665 - acc: 0.7062 - val_loss: 1.5020 - val_acc: 0.6570
Epoch 29/100
 - 83s - loss: 1.0250 - acc: 0.7173 - val_loss: 1.6290 - val_acc: 0.6378
Epoch 30/100
 - 85s - loss: 1.0065 - acc: 0.7251 - val_loss: 1.3912 - val_acc: 0.6700
Epoch 31/100
 - 87s - loss: 0.9798 - acc: 0.7311 - val_loss: 1.6036 - val_acc: 0.6548
Epoch 32/100
 - 88s - loss: 0.9756 - acc: 0.7329 - val_loss: 1.6182 - val_acc: 0.6525
Epoch 33/100
 - 88s - loss: 0.9238 - acc: 0.7465 - val_loss: 1.8045 - val_acc: 0.6405
Epoch 34/100
 - 89s - loss: 0.9132 - acc: 0.7514 - val_loss: 1.3934 - val_acc: 0.6862
Epoch 35/100
 - 89s - loss: 0.8871 - acc: 0.7584 - val_loss: 1.5409 - val_acc: 0.6740
Epoch 36/100
 - 89s - loss: 0.8809 - acc: 0.7605 - val_loss: 1.6246 - val_acc: 0.6613
Epoch 37/100
 - 90s - loss: 0.8529 - acc: 0.7703 - val_loss: 1.4674 - val_acc: 0.6677
Epoch 38/100
 - 89s - loss: 0.8362 - acc: 0.7735 - val_loss: 1.6527 - val_acc: 0.6615
Epoch 39/100
 - 88s - loss: 0.8195 - acc: 0.7774 - val_loss: 1.4942 - val_acc: 0.6743
Epoch 40/100
 - 88s - loss: 0.8215 - acc: 0.7781 - val_loss: 1.6228 - val_acc: 0.6645
Epoch 41/100
 - 89s - loss: 0.8089 - acc: 0.7797 - val_loss: 1.5341 - val_acc: 0.6790
Epoch 42/100
 - 89s - loss: 0.7860 - acc: 0.7877 - val_loss: 1.6708 - val_acc: 0.6683
Epoch 43/100
 - 89s - loss: 0.7750 - acc: 0.7903 - val_loss: 1.5916 - val_acc: 0.6667
Epoch 44/100
 - 89s - loss: 0.7609 - acc: 0.7943 - val_loss: 1.6802 - val_acc: 0.6667
Epoch 45/100
 - 89s - loss: 0.7579 - acc: 0.7946 - val_loss: 1.5197 - val_acc: 0.6850
Epoch 46/100
 - 89s - loss: 0.7358 - acc: 0.8000 - val_loss: 1.6353 - val_acc: 0.6655
Epoch 47/100
 - 89s - loss: 0.7271 - acc: 0.8043 - val_loss: 1.5862 - val_acc: 0.6822
Epoch 48/100
 - 89s - loss: 0.7210 - acc: 0.8046 - val_loss: 1.5469 - val_acc: 0.6850
Epoch 49/100
 - 89s - loss: 0.7167 - acc: 0.8070 - val_loss: 1.6037 - val_acc: 0.6922
Epoch 50/100
 - 89s - loss: 0.7062 - acc: 0.8095 - val_loss: 1.6501 - val_acc: 0.6800
Epoch 51/100
 - 89s - loss: 0.6909 - acc: 0.8118 - val_loss: 1.6255 - val_acc: 0.6818
Epoch 52/100
 - 90s - loss: 0.6916 - acc: 0.8111 - val_loss: 1.5617 - val_acc: 0.6890
Epoch 53/100
 - 89s - loss: 0.6848 - acc: 0.8143 - val_loss: 1.7641 - val_acc: 0.6602
Epoch 54/100
 - 88s - loss: 0.6731 - acc: 0.8183 - val_loss: 1.5695 - val_acc: 0.6837
Epoch 55/100
 - 88s - loss: 0.6863 - acc: 0.8154 - val_loss: 1.5940 - val_acc: 0.6847
Epoch 56/100
 - 88s - loss: 0.6615 - acc: 0.8203 - val_loss: 1.4827 - val_acc: 0.6955
Epoch 57/100
 - 87s - loss: 0.6586 - acc: 0.8232 - val_loss: 1.6040 - val_acc: 0.6907
Epoch 58/100
 - 88s - loss: 0.6610 - acc: 0.8208 - val_loss: 1.8094 - val_acc: 0.6718
Epoch 59/100
 - 87s - loss: 0.6628 - acc: 0.8213 - val_loss: 1.8774 - val_acc: 0.6465
Epoch 60/100
 - 88s - loss: 0.6410 - acc: 0.8260 - val_loss: 1.6216 - val_acc: 0.6998
Epoch 61/100
 - 87s - loss: 0.6355 - acc: 0.8288 - val_loss: 1.6689 - val_acc: 0.6755
Epoch 62/100
 - 88s - loss: 0.6194 - acc: 0.8325 - val_loss: 1.5420 - val_acc: 0.6985
Epoch 63/100
 - 88s - loss: 0.6139 - acc: 0.8331 - val_loss: 1.7155 - val_acc: 0.6790
Epoch 64/100
 - 88s - loss: 0.5945 - acc: 0.8388 - val_loss: 1.6625 - val_acc: 0.6893
Epoch 65/100
 - 88s - loss: 0.6070 - acc: 0.8362 - val_loss: 1.5527 - val_acc: 0.6972
Epoch 66/100
 - 88s - loss: 0.6047 - acc: 0.8374 - val_loss: 1.6473 - val_acc: 0.6995
Epoch 67/100
 - 88s - loss: 0.6103 - acc: 0.8363 - val_loss: 1.5853 - val_acc: 0.7000
Epoch 68/100
 - 88s - loss: 0.6008 - acc: 0.8394 - val_loss: 1.6113 - val_acc: 0.7045
Epoch 69/100
 - 88s - loss: 0.5851 - acc: 0.8423 - val_loss: 1.7587 - val_acc: 0.6838
Epoch 70/100
 - 89s - loss: 0.5965 - acc: 0.8374 - val_loss: 1.7217 - val_acc: 0.6878
Epoch 71/100
 - 88s - loss: 0.5717 - acc: 0.8439 - val_loss: 1.6724 - val_acc: 0.6915
Epoch 72/100
 - 89s - loss: 0.5685 - acc: 0.8473 - val_loss: 1.8719 - val_acc: 0.6787
Epoch 73/100
 - 89s - loss: 0.5757 - acc: 0.8437 - val_loss: 1.6581 - val_acc: 0.6953
Epoch 74/100
 - 89s - loss: 0.5587 - acc: 0.8493 - val_loss: 1.5717 - val_acc: 0.6987
Epoch 75/100
 - 89s - loss: 0.5676 - acc: 0.8457 - val_loss: 1.7275 - val_acc: 0.6920
Epoch 76/100
 - 90s - loss: 0.5414 - acc: 0.8527 - val_loss: 1.7060 - val_acc: 0.6947
Epoch 77/100
 - 87s - loss: 0.5634 - acc: 0.8467 - val_loss: 1.7159 - val_acc: 0.6937
Epoch 78/100
 - 88s - loss: 0.5452 - acc: 0.8519 - val_loss: 1.6350 - val_acc: 0.7050
Epoch 79/100
 - 88s - loss: 0.5470 - acc: 0.8517 - val_loss: 1.8296 - val_acc: 0.6708
Epoch 80/100
 - 89s - loss: 0.5403 - acc: 0.8542 - val_loss: 1.7925 - val_acc: 0.6902
Epoch 81/100
 - 88s - loss: 0.5351 - acc: 0.8532 - val_loss: 1.6526 - val_acc: 0.6943
Epoch 82/100
 - 88s - loss: 0.5396 - acc: 0.8532 - val_loss: 1.6977 - val_acc: 0.6940
Epoch 83/100
 - 87s - loss: 0.5315 - acc: 0.8559 - val_loss: 1.6574 - val_acc: 0.6840
Epoch 84/100
 - 87s - loss: 0.5267 - acc: 0.8568 - val_loss: 1.6690 - val_acc: 0.6988
Epoch 85/100
 - 87s - loss: 0.5275 - acc: 0.8570 - val_loss: 1.7504 - val_acc: 0.6893
Epoch 86/100
 - 88s - loss: 0.5109 - acc: 0.8604 - val_loss: 1.6873 - val_acc: 0.6940
Epoch 87/100
 - 89s - loss: 0.5190 - acc: 0.8595 - val_loss: 1.7415 - val_acc: 0.6790
Epoch 88/100
 - 89s - loss: 0.5192 - acc: 0.8602 - val_loss: 1.9059 - val_acc: 0.6888
Epoch 89/100
 - 89s - loss: 0.5136 - acc: 0.8599 - val_loss: 1.7568 - val_acc: 0.6933
Epoch 90/100
 - 89s - loss: 0.5035 - acc: 0.8632 - val_loss: 1.6496 - val_acc: 0.7042
Epoch 91/100
 - 89s - loss: 0.5003 - acc: 0.8642 - val_loss: 1.6283 - val_acc: 0.7058
Epoch 92/100
 - 89s - loss: 0.5114 - acc: 0.8631 - val_loss: 1.6804 - val_acc: 0.7050
Epoch 93/100
 - 89s - loss: 0.5084 - acc: 0.8616 - val_loss: 1.7266 - val_acc: 0.6927
Epoch 94/100
 - 89s - loss: 0.4809 - acc: 0.8696 - val_loss: 1.6466 - val_acc: 0.7035
Epoch 95/100
 - 88s - loss: 0.5013 - acc: 0.8651 - val_loss: 1.6393 - val_acc: 0.7063
Epoch 96/100
 - 89s - loss: 0.4932 - acc: 0.8670 - val_loss: 1.7391 - val_acc: 0.7082
Epoch 97/100
 - 88s - loss: 0.4919 - acc: 0.8666 - val_loss: 1.5948 - val_acc: 0.7005
Epoch 98/100
 - 88s - loss: 0.4890 - acc: 0.8695 - val_loss: 1.7309 - val_acc: 0.6992
Epoch 99/100
 - 88s - loss: 0.4817 - acc: 0.8694 - val_loss: 1.6658 - val_acc: 0.7120
Epoch 100/100
 - 88s - loss: 0.4842 - acc: 0.8693 - val_loss: 1.7608 - val_acc: 0.6897
Reached validation accuracy is 0.6896666671435038
8867.978481769562
