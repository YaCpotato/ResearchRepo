wrn_28_10 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 100)          64100       global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,564,932
Trainable params: 36,546,980
Non-trainable params: 17,952
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.894, 'aug2_type': 'crop', 'aug2_magnitude': 0.163}, {'aug1_type': 'translate-y', 'aug1_magnitude': 0.91, 'aug2_type': 'RandomEracing', 'aug2_magnitude': 0.042}, {'aug1_type': 'invert', 'aug1_magnitude': 0.937, 'aug2_type': 'invert', 'aug2_magnitude': 0.484}, {'aug1_type': 'RandomEracing', 'aug1_magnitude': 0.079, 'aug2_type': 'RandomEracing', 'aug2_magnitude': 0.001}, {'aug1_type': 'emboss', 'aug1_magnitude': 0.015, 'aug2_type': 'RandomEracing', 'aug2_magnitude': 0.24600000000000002}]

 - 99s - loss: 4.3559 - acc: 0.0791 - val_loss: 4.2242 - val_acc: 0.0927
Epoch 2/100
 - 89s - loss: 3.6132 - acc: 0.1670 - val_loss: 3.9060 - val_acc: 0.1360
Epoch 3/100
 - 89s - loss: 3.2118 - acc: 0.2383 - val_loss: 3.2560 - val_acc: 0.2417
Epoch 4/100
 - 89s - loss: 2.8732 - acc: 0.3010 - val_loss: 2.9513 - val_acc: 0.3005
Epoch 5/100
 - 89s - loss: 2.5772 - acc: 0.3628 - val_loss: 2.7140 - val_acc: 0.3365
Epoch 6/100
 - 89s - loss: 2.3478 - acc: 0.4090 - val_loss: 2.3405 - val_acc: 0.4078
Epoch 7/100
 - 90s - loss: 2.1354 - acc: 0.4541 - val_loss: 2.3358 - val_acc: 0.4162
Epoch 8/100
 - 89s - loss: 1.9954 - acc: 0.4874 - val_loss: 1.9434 - val_acc: 0.4938
Epoch 9/100
 - 89s - loss: 1.8663 - acc: 0.5195 - val_loss: 2.0286 - val_acc: 0.4748
Epoch 10/100
 - 89s - loss: 1.7007 - acc: 0.5548 - val_loss: 2.2271 - val_acc: 0.4670
Epoch 11/100
 - 89s - loss: 1.6209 - acc: 0.5764 - val_loss: 2.0678 - val_acc: 0.4778
Epoch 12/100
 - 89s - loss: 1.5086 - acc: 0.6045 - val_loss: 1.5953 - val_acc: 0.5635
Epoch 13/100
 - 88s - loss: 1.4231 - acc: 0.6176 - val_loss: 1.7966 - val_acc: 0.5413
Epoch 14/100
 - 88s - loss: 1.2687 - acc: 0.6485 - val_loss: 2.2368 - val_acc: 0.5545
Epoch 15/100
 - 88s - loss: 1.1769 - acc: 0.6675 - val_loss: 1.5772 - val_acc: 0.5960
Epoch 16/100
 - 89s - loss: 1.0903 - acc: 0.6883 - val_loss: 1.7361 - val_acc: 0.5707
Epoch 17/100
 - 88s - loss: 1.0070 - acc: 0.7125 - val_loss: 1.4400 - val_acc: 0.6233
Epoch 18/100
 - 86s - loss: 0.9490 - acc: 0.7269 - val_loss: 1.4471 - val_acc: 0.6225
Epoch 19/100
 - 89s - loss: 0.8697 - acc: 0.7505 - val_loss: 1.4850 - val_acc: 0.6365
Epoch 20/100
 - 88s - loss: 0.8227 - acc: 0.7632 - val_loss: 1.4910 - val_acc: 0.6320
Epoch 21/100
 - 86s - loss: 0.7614 - acc: 0.7791 - val_loss: 1.4960 - val_acc: 0.6373
Epoch 22/100
 - 88s - loss: 0.7297 - acc: 0.7885 - val_loss: 1.5410 - val_acc: 0.6218
Epoch 23/100
 - 88s - loss: 0.6911 - acc: 0.8009 - val_loss: 1.5609 - val_acc: 0.6518
Epoch 24/100
 - 88s - loss: 0.6564 - acc: 0.8104 - val_loss: 1.8011 - val_acc: 0.6240
Epoch 25/100
 - 89s - loss: 0.6266 - acc: 0.8194 - val_loss: 1.9112 - val_acc: 0.6220
Epoch 26/100
 - 89s - loss: 0.5943 - acc: 0.8281 - val_loss: 1.8475 - val_acc: 0.6147
Epoch 27/100
 - 89s - loss: 0.5760 - acc: 0.8344 - val_loss: 1.5546 - val_acc: 0.6577
Epoch 28/100
 - 89s - loss: 0.5552 - acc: 0.8403 - val_loss: 1.7232 - val_acc: 0.6357
Epoch 29/100
 - 88s - loss: 0.5328 - acc: 0.8481 - val_loss: 1.6425 - val_acc: 0.6483
Epoch 30/100
 - 89s - loss: 0.5163 - acc: 0.8531 - val_loss: 1.7981 - val_acc: 0.6347
Epoch 31/100
 - 89s - loss: 0.5038 - acc: 0.8565 - val_loss: 1.5276 - val_acc: 0.6777
Epoch 32/100
 - 89s - loss: 0.4798 - acc: 0.8644 - val_loss: 1.7843 - val_acc: 0.6440
Epoch 33/100
 - 88s - loss: 0.4741 - acc: 0.8646 - val_loss: 1.8128 - val_acc: 0.6555
Epoch 34/100
 - 88s - loss: 0.4798 - acc: 0.8651 - val_loss: 1.7020 - val_acc: 0.6713
Epoch 35/100
 - 88s - loss: 0.4580 - acc: 0.8716 - val_loss: 1.7763 - val_acc: 0.6625
Epoch 36/100
 - 90s - loss: 0.4584 - acc: 0.8722 - val_loss: 1.6834 - val_acc: 0.6683
Epoch 37/100
 - 90s - loss: 0.4335 - acc: 0.8780 - val_loss: 1.9700 - val_acc: 0.6322
Epoch 38/100
 - 89s - loss: 0.4251 - acc: 0.8801 - val_loss: 1.8519 - val_acc: 0.6597
Epoch 39/100
 - 89s - loss: 0.4230 - acc: 0.8808 - val_loss: 1.8921 - val_acc: 0.6397
Epoch 40/100
 - 89s - loss: 0.4080 - acc: 0.8850 - val_loss: 2.0138 - val_acc: 0.6490
Epoch 41/100
 - 89s - loss: 0.3934 - acc: 0.8900 - val_loss: 1.8248 - val_acc: 0.6542
Epoch 42/100
 - 89s - loss: 0.3978 - acc: 0.8882 - val_loss: 1.7993 - val_acc: 0.6597
Epoch 43/100
 - 88s - loss: 0.4043 - acc: 0.8878 - val_loss: 1.8119 - val_acc: 0.6628
Epoch 44/100
 - 88s - loss: 0.3942 - acc: 0.8913 - val_loss: 1.9545 - val_acc: 0.6442
Epoch 45/100
 - 88s - loss: 0.3809 - acc: 0.8938 - val_loss: 1.8154 - val_acc: 0.6532
Epoch 46/100
 - 89s - loss: 0.3811 - acc: 0.8944 - val_loss: 1.6647 - val_acc: 0.6798
Epoch 47/100
 - 89s - loss: 0.3711 - acc: 0.8967 - val_loss: 1.8048 - val_acc: 0.6813
Epoch 48/100
 - 89s - loss: 0.3701 - acc: 0.8975 - val_loss: 1.7334 - val_acc: 0.6845
Epoch 49/100
 - 89s - loss: 0.3556 - acc: 0.9006 - val_loss: 1.7207 - val_acc: 0.6773
Epoch 50/100
 - 89s - loss: 0.3477 - acc: 0.9044 - val_loss: 1.7283 - val_acc: 0.6798
Epoch 51/100
 - 89s - loss: 0.3510 - acc: 0.9017 - val_loss: 1.8969 - val_acc: 0.6652
Epoch 52/100
 - 89s - loss: 0.3522 - acc: 0.9031 - val_loss: 1.8608 - val_acc: 0.6698
Epoch 53/100
 - 89s - loss: 0.3429 - acc: 0.9057 - val_loss: 1.9212 - val_acc: 0.6763
Epoch 54/100
 - 89s - loss: 0.3420 - acc: 0.9049 - val_loss: 1.8640 - val_acc: 0.6880
Epoch 55/100
 - 89s - loss: 0.3393 - acc: 0.9064 - val_loss: 1.7070 - val_acc: 0.6872
Epoch 56/100
 - 87s - loss: 0.3304 - acc: 0.9086 - val_loss: 1.8699 - val_acc: 0.6853
Epoch 57/100
 - 88s - loss: 0.3189 - acc: 0.9116 - val_loss: 1.8220 - val_acc: 0.6913
Epoch 58/100
 - 88s - loss: 0.3176 - acc: 0.9127 - val_loss: 1.8647 - val_acc: 0.6818
Epoch 59/100
 - 88s - loss: 0.3218 - acc: 0.9124 - val_loss: 1.6877 - val_acc: 0.6850
Epoch 60/100
 - 88s - loss: 0.3158 - acc: 0.9129 - val_loss: 1.7989 - val_acc: 0.6780
Epoch 61/100
 - 89s - loss: 0.3167 - acc: 0.9134 - val_loss: 1.9862 - val_acc: 0.6608
Epoch 62/100
 - 89s - loss: 0.3167 - acc: 0.9136 - val_loss: 1.8194 - val_acc: 0.6843
Epoch 63/100
 - 88s - loss: 0.3115 - acc: 0.9157 - val_loss: 1.8520 - val_acc: 0.6748
Epoch 64/100
 - 88s - loss: 0.3072 - acc: 0.9165 - val_loss: 1.8099 - val_acc: 0.6795
Epoch 65/100
 - 88s - loss: 0.3002 - acc: 0.9168 - val_loss: 1.9195 - val_acc: 0.6797
Epoch 66/100
 - 88s - loss: 0.3031 - acc: 0.9169 - val_loss: 2.1253 - val_acc: 0.6597
Epoch 67/100
 - 88s - loss: 0.2996 - acc: 0.9188 - val_loss: 2.0773 - val_acc: 0.6690
Epoch 68/100
 - 88s - loss: 0.2974 - acc: 0.9183 - val_loss: 1.8787 - val_acc: 0.6862
Epoch 69/100
 - 89s - loss: 0.2997 - acc: 0.9181 - val_loss: 1.8627 - val_acc: 0.6767
Epoch 70/100
 - 89s - loss: 0.2945 - acc: 0.9189 - val_loss: 1.8435 - val_acc: 0.6913
Epoch 71/100
 - 89s - loss: 0.2994 - acc: 0.9183 - val_loss: 1.6467 - val_acc: 0.7083
Epoch 72/100
 - 88s - loss: 0.2926 - acc: 0.9209 - val_loss: 2.0196 - val_acc: 0.6775
Epoch 73/100
 - 88s - loss: 0.2908 - acc: 0.9204 - val_loss: 1.9476 - val_acc: 0.6803
Epoch 74/100
 - 89s - loss: 0.2853 - acc: 0.9221 - val_loss: 2.0177 - val_acc: 0.6745
Epoch 75/100
 - 89s - loss: 0.2824 - acc: 0.9236 - val_loss: 1.9508 - val_acc: 0.6858
Epoch 76/100
 - 89s - loss: 0.2793 - acc: 0.9232 - val_loss: 1.8975 - val_acc: 0.6985
Epoch 77/100
 - 88s - loss: 0.2769 - acc: 0.9243 - val_loss: 1.8899 - val_acc: 0.6887
Epoch 78/100
 - 89s - loss: 0.2835 - acc: 0.9235 - val_loss: 1.7910 - val_acc: 0.6953
Epoch 79/100
 - 88s - loss: 0.2693 - acc: 0.9259 - val_loss: 1.8603 - val_acc: 0.6847
Epoch 80/100
 - 89s - loss: 0.2766 - acc: 0.9238 - val_loss: 1.9326 - val_acc: 0.6932
Epoch 81/100
 - 88s - loss: 0.2744 - acc: 0.9251 - val_loss: 2.1110 - val_acc: 0.6652
Epoch 82/100
 - 89s - loss: 0.2707 - acc: 0.9269 - val_loss: 2.0677 - val_acc: 0.6618
Epoch 83/100
 - 89s - loss: 0.2654 - acc: 0.9273 - val_loss: 2.0155 - val_acc: 0.6853
Epoch 84/100
 - 89s - loss: 0.2640 - acc: 0.9283 - val_loss: 1.7974 - val_acc: 0.6967
Epoch 85/100
 - 88s - loss: 0.2621 - acc: 0.9292 - val_loss: 2.0712 - val_acc: 0.6745
Epoch 86/100
 - 88s - loss: 0.2627 - acc: 0.9283 - val_loss: 1.9233 - val_acc: 0.6948
Epoch 87/100
 - 88s - loss: 0.2738 - acc: 0.9260 - val_loss: 1.8662 - val_acc: 0.6903
Epoch 88/100
 - 89s - loss: 0.2656 - acc: 0.9285 - val_loss: 1.9102 - val_acc: 0.6892
Epoch 89/100
 - 89s - loss: 0.2618 - acc: 0.9300 - val_loss: 2.2235 - val_acc: 0.6558
Epoch 90/100
 - 90s - loss: 0.2683 - acc: 0.9273 - val_loss: 1.8505 - val_acc: 0.6932
Epoch 91/100
 - 89s - loss: 0.2565 - acc: 0.9304 - val_loss: 1.8932 - val_acc: 0.7030
Epoch 92/100
 - 89s - loss: 0.2518 - acc: 0.9312 - val_loss: 1.8362 - val_acc: 0.6893
Epoch 93/100
 - 89s - loss: 0.2605 - acc: 0.9292 - val_loss: 1.8908 - val_acc: 0.6942
Epoch 94/100
 - 88s - loss: 0.2521 - acc: 0.9312 - val_loss: 1.9498 - val_acc: 0.6973
Epoch 95/100
 - 89s - loss: 0.2670 - acc: 0.9275 - val_loss: 1.8466 - val_acc: 0.7037
Epoch 96/100
 - 88s - loss: 0.2479 - acc: 0.9327 - val_loss: 2.0197 - val_acc: 0.6723
Epoch 97/100
 - 89s - loss: 0.2598 - acc: 0.9299 - val_loss: 1.7457 - val_acc: 0.6988
Epoch 98/100
 - 89s - loss: 0.2602 - acc: 0.9297 - val_loss: 1.8612 - val_acc: 0.6927
Epoch 99/100
 - 87s - loss: 0.2417 - acc: 0.9339 - val_loss: 2.1361 - val_acc: 0.6812
Epoch 100/100
 - 87s - loss: 0.2389 - acc: 0.9353 - val_loss: 1.8953 - val_acc: 0.6902
Reached validation accuracy is 0.6901666666666667
8883.217943668365
