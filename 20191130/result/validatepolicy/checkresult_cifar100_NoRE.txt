wrn_28_10 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 100)          64100       global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,564,932
Trainable params: 36,546,980
Non-trainable params: 17,952
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'add-to-hue-and-saturation', 'aug1_magnitude': 0.445, 'aug2_type': 'additive-gaussian-noise', 'aug2_magnitude': 0.008}, {'aug1_type': 'gamma-contrast', 'aug1_magnitude': 0.884, 'aug2_type': 'vertical-flip', 'aug2_magnitude': 0.875}, {'aug1_type': 'additive-gaussian-noise', 'aug1_magnitude': 0.006999999999999999, 'aug2_type': 'shear', 'aug2_magnitude': 0.204}, {'aug1_type': 'brighten', 'aug1_magnitude': 0.057999999999999996, 'aug2_type': 'brighten', 'aug2_magnitude': 0.85}, {'aug1_type': 'shear', 'aug1_magnitude': 0.054000000000000006, 'aug2_type': 'brighten', 'aug2_magnitude': 0.035}]

 - 99s - loss: 4.4443 - acc: 0.0655 - val_loss: 4.0741 - val_acc: 0.0757
Epoch 2/100
 - 88s - loss: 3.8900 - acc: 0.1285 - val_loss: 3.3863 - val_acc: 0.1875
Epoch 3/100
 - 87s - loss: 3.5345 - acc: 0.1838 - val_loss: 3.0690 - val_acc: 0.2535
Epoch 4/100
 - 88s - loss: 3.2376 - acc: 0.2359 - val_loss: 2.8566 - val_acc: 0.2897
Epoch 5/100
 - 88s - loss: 2.9975 - acc: 0.2840 - val_loss: 2.8999 - val_acc: 0.2898
Epoch 6/100
 - 88s - loss: 2.8230 - acc: 0.3235 - val_loss: 2.3946 - val_acc: 0.3788
Epoch 7/100
 - 88s - loss: 2.6245 - acc: 0.3605 - val_loss: 2.2657 - val_acc: 0.4062
Epoch 8/100
 - 88s - loss: 2.4721 - acc: 0.3946 - val_loss: 2.1076 - val_acc: 0.4448
Epoch 9/100
 - 88s - loss: 2.3441 - acc: 0.4273 - val_loss: 2.2091 - val_acc: 0.4355
Epoch 10/100
 - 87s - loss: 2.2421 - acc: 0.4519 - val_loss: 2.0699 - val_acc: 0.4538
Epoch 11/100
 - 88s - loss: 2.1523 - acc: 0.4700 - val_loss: 2.1028 - val_acc: 0.4775
Epoch 12/100
 - 89s - loss: 2.0638 - acc: 0.4887 - val_loss: 1.8189 - val_acc: 0.5197
Epoch 13/100
 - 88s - loss: 1.9884 - acc: 0.5081 - val_loss: 2.2323 - val_acc: 0.4602
Epoch 14/100
 - 88s - loss: 1.9161 - acc: 0.5242 - val_loss: 1.5907 - val_acc: 0.5615
Epoch 15/100
 - 88s - loss: 1.8233 - acc: 0.5484 - val_loss: 1.6951 - val_acc: 0.5538
Epoch 16/100
 - 88s - loss: 1.7667 - acc: 0.5655 - val_loss: 1.5302 - val_acc: 0.5798
Epoch 17/100
 - 89s - loss: 1.6966 - acc: 0.5828 - val_loss: 1.9177 - val_acc: 0.5412
Epoch 18/100
 - 89s - loss: 1.6546 - acc: 0.5910 - val_loss: 1.7121 - val_acc: 0.5733
Epoch 19/100
 - 89s - loss: 1.5319 - acc: 0.6032 - val_loss: 1.4801 - val_acc: 0.6072
Epoch 20/100
 - 89s - loss: 1.4445 - acc: 0.6223 - val_loss: 1.4590 - val_acc: 0.6185
Epoch 21/100
 - 89s - loss: 1.3901 - acc: 0.6375 - val_loss: 1.5304 - val_acc: 0.6127
Epoch 22/100
 - 89s - loss: 1.3228 - acc: 0.6552 - val_loss: 1.3959 - val_acc: 0.6307
Epoch 23/100
 - 88s - loss: 1.2944 - acc: 0.6653 - val_loss: 1.6777 - val_acc: 0.5925
Epoch 24/100
 - 85s - loss: 1.2482 - acc: 0.6774 - val_loss: 1.5310 - val_acc: 0.6215
Epoch 25/100
 - 85s - loss: 1.2179 - acc: 0.6843 - val_loss: 1.4436 - val_acc: 0.6425
Epoch 26/100
 - 87s - loss: 1.1437 - acc: 0.7069 - val_loss: 1.6233 - val_acc: 0.6163
Epoch 27/100
 - 88s - loss: 1.1234 - acc: 0.7110 - val_loss: 1.7493 - val_acc: 0.5998
Epoch 28/100
 - 88s - loss: 1.1110 - acc: 0.7160 - val_loss: 1.6773 - val_acc: 0.6295
Epoch 29/100
 - 89s - loss: 1.0827 - acc: 0.7244 - val_loss: 1.6187 - val_acc: 0.6312
Epoch 30/100
 - 89s - loss: 1.0649 - acc: 0.7304 - val_loss: 1.6995 - val_acc: 0.6375
Epoch 31/100
 - 89s - loss: 1.0366 - acc: 0.7375 - val_loss: 2.1474 - val_acc: 0.5662
Epoch 32/100
 - 89s - loss: 1.0205 - acc: 0.7447 - val_loss: 1.7739 - val_acc: 0.6215
Epoch 33/100
 - 89s - loss: 0.9883 - acc: 0.7502 - val_loss: 2.0446 - val_acc: 0.6245
Epoch 34/100
 - 89s - loss: 0.9980 - acc: 0.7518 - val_loss: 1.8021 - val_acc: 0.6408
Epoch 35/100
 - 89s - loss: 0.9303 - acc: 0.7662 - val_loss: 1.6641 - val_acc: 0.6503
Epoch 36/100
 - 89s - loss: 0.9606 - acc: 0.7627 - val_loss: 1.5962 - val_acc: 0.6493
Epoch 37/100
 - 89s - loss: 0.9336 - acc: 0.7686 - val_loss: 1.6506 - val_acc: 0.6527
Epoch 38/100
 - 89s - loss: 0.9174 - acc: 0.7727 - val_loss: 1.8480 - val_acc: 0.6378
Epoch 39/100
 - 89s - loss: 0.8990 - acc: 0.7792 - val_loss: 1.7755 - val_acc: 0.6522
Epoch 40/100
 - 89s - loss: 0.8990 - acc: 0.7797 - val_loss: 1.8848 - val_acc: 0.6412
Epoch 41/100
 - 89s - loss: 0.8698 - acc: 0.7854 - val_loss: 1.4689 - val_acc: 0.6760
Epoch 42/100
 - 89s - loss: 0.8656 - acc: 0.7872 - val_loss: 1.7812 - val_acc: 0.6570
Epoch 43/100
 - 89s - loss: 0.8832 - acc: 0.7850 - val_loss: 1.5688 - val_acc: 0.6672
Epoch 44/100
 - 89s - loss: 0.8467 - acc: 0.7937 - val_loss: 1.9779 - val_acc: 0.6330
Epoch 45/100
 - 89s - loss: 0.8590 - acc: 0.7904 - val_loss: 2.0241 - val_acc: 0.6372
Epoch 46/100
 - 89s - loss: 0.8369 - acc: 0.7960 - val_loss: 1.7387 - val_acc: 0.6720
Epoch 47/100
 - 88s - loss: 0.8407 - acc: 0.7965 - val_loss: 1.8829 - val_acc: 0.6407
Epoch 48/100
 - 87s - loss: 0.8220 - acc: 0.8013 - val_loss: 1.9459 - val_acc: 0.6433
Epoch 49/100
 - 89s - loss: 0.8334 - acc: 0.7963 - val_loss: 1.6315 - val_acc: 0.6718
Epoch 50/100
 - 89s - loss: 0.8329 - acc: 0.8007 - val_loss: 1.8966 - val_acc: 0.6543
Epoch 51/100
 - 88s - loss: 0.7867 - acc: 0.8097 - val_loss: 1.8887 - val_acc: 0.6602
Epoch 52/100
 - 89s - loss: 0.8296 - acc: 0.8000 - val_loss: 1.9477 - val_acc: 0.6573
Epoch 53/100
 - 89s - loss: 0.8005 - acc: 0.8071 - val_loss: 1.9946 - val_acc: 0.6367
Epoch 54/100
 - 89s - loss: 0.7800 - acc: 0.8126 - val_loss: 1.8478 - val_acc: 0.6602
Epoch 55/100
 - 90s - loss: 0.7648 - acc: 0.8154 - val_loss: 1.7638 - val_acc: 0.6645
Epoch 56/100
 - 89s - loss: 0.7870 - acc: 0.8106 - val_loss: 1.6075 - val_acc: 0.6835
Epoch 57/100
 - 89s - loss: 0.7677 - acc: 0.8169 - val_loss: 1.9089 - val_acc: 0.6660
Epoch 58/100
 - 89s - loss: 0.7645 - acc: 0.8180 - val_loss: 1.7178 - val_acc: 0.6870
Epoch 59/100
 - 88s - loss: 0.7745 - acc: 0.8142 - val_loss: 1.7217 - val_acc: 0.6810
Epoch 60/100
 - 88s - loss: 0.7840 - acc: 0.8130 - val_loss: 1.7984 - val_acc: 0.6673
Epoch 61/100
 - 88s - loss: 0.7717 - acc: 0.8160 - val_loss: 2.0451 - val_acc: 0.6542
Epoch 62/100
 - 88s - loss: 0.7687 - acc: 0.8175 - val_loss: 2.0716 - val_acc: 0.6418
Epoch 63/100
 - 89s - loss: 0.7726 - acc: 0.8167 - val_loss: 1.9413 - val_acc: 0.6580
Epoch 64/100
 - 88s - loss: 0.7473 - acc: 0.8236 - val_loss: 1.8948 - val_acc: 0.6663
Epoch 65/100
 - 89s - loss: 0.7716 - acc: 0.8171 - val_loss: 1.9455 - val_acc: 0.6625
Epoch 66/100
 - 89s - loss: 0.7537 - acc: 0.8214 - val_loss: 1.9482 - val_acc: 0.6460
Epoch 67/100
 - 89s - loss: 0.7527 - acc: 0.8216 - val_loss: 2.0407 - val_acc: 0.6597
Epoch 68/100
 - 89s - loss: 0.7399 - acc: 0.8242 - val_loss: 1.9844 - val_acc: 0.6498
Epoch 69/100
 - 88s - loss: 0.7539 - acc: 0.8209 - val_loss: 1.8886 - val_acc: 0.6695
Epoch 70/100
 - 89s - loss: 0.7190 - acc: 0.8308 - val_loss: 2.0261 - val_acc: 0.6733
Epoch 71/100
 - 89s - loss: 0.7462 - acc: 0.8233 - val_loss: 2.0578 - val_acc: 0.6520
Epoch 72/100
 - 89s - loss: 0.7369 - acc: 0.8262 - val_loss: 2.1244 - val_acc: 0.6612
Epoch 73/100
 - 89s - loss: 0.7449 - acc: 0.8255 - val_loss: 1.9035 - val_acc: 0.6698
Epoch 74/100
 - 88s - loss: 0.7327 - acc: 0.8291 - val_loss: 2.1477 - val_acc: 0.6522
Epoch 75/100
 - 88s - loss: 0.7227 - acc: 0.8298 - val_loss: 1.8754 - val_acc: 0.6770
Epoch 76/100
 - 88s - loss: 0.7110 - acc: 0.8318 - val_loss: 2.0966 - val_acc: 0.6533
Epoch 77/100
 - 89s - loss: 0.6894 - acc: 0.8375 - val_loss: 2.0658 - val_acc: 0.6637
Epoch 78/100
 - 89s - loss: 0.6986 - acc: 0.8350 - val_loss: 2.0583 - val_acc: 0.6613
Epoch 79/100
 - 89s - loss: 0.7208 - acc: 0.8319 - val_loss: 1.9047 - val_acc: 0.6735
Epoch 80/100
 - 88s - loss: 0.7099 - acc: 0.8339 - val_loss: 1.8613 - val_acc: 0.6822
Epoch 81/100
 - 89s - loss: 0.7237 - acc: 0.8308 - val_loss: 2.0834 - val_acc: 0.6518
Epoch 82/100
 - 89s - loss: 0.7087 - acc: 0.8338 - val_loss: 2.0305 - val_acc: 0.6712
Epoch 83/100
 - 89s - loss: 0.7077 - acc: 0.8342 - val_loss: 2.0869 - val_acc: 0.6637
Epoch 84/100
 - 89s - loss: 0.7042 - acc: 0.8344 - val_loss: 1.9777 - val_acc: 0.6613
Epoch 85/100
 - 89s - loss: 0.6964 - acc: 0.8365 - val_loss: 2.0253 - val_acc: 0.6710
Epoch 86/100
 - 89s - loss: 0.7007 - acc: 0.8350 - val_loss: 2.0909 - val_acc: 0.6602
Epoch 87/100
 - 89s - loss: 0.6946 - acc: 0.8365 - val_loss: 1.8760 - val_acc: 0.6830
Epoch 88/100
 - 89s - loss: 0.7027 - acc: 0.8370 - val_loss: 2.0645 - val_acc: 0.6658
Epoch 89/100
 - 89s - loss: 0.7048 - acc: 0.8364 - val_loss: 1.8624 - val_acc: 0.6858
Epoch 90/100
 - 89s - loss: 0.6790 - acc: 0.8413 - val_loss: 2.0496 - val_acc: 0.6802
Epoch 91/100
 - 89s - loss: 0.6973 - acc: 0.8374 - val_loss: 1.7870 - val_acc: 0.6873
Epoch 92/100
 - 89s - loss: 0.6648 - acc: 0.8449 - val_loss: 2.1023 - val_acc: 0.6723
Epoch 93/100
 - 89s - loss: 0.7003 - acc: 0.8372 - val_loss: 2.3532 - val_acc: 0.6620
Epoch 94/100
 - 89s - loss: 0.6662 - acc: 0.8441 - val_loss: 2.0732 - val_acc: 0.6590
Epoch 95/100
 - 89s - loss: 0.6954 - acc: 0.8373 - val_loss: 1.9445 - val_acc: 0.6868
Epoch 96/100
 - 89s - loss: 0.6969 - acc: 0.8376 - val_loss: 2.1287 - val_acc: 0.6653
Epoch 97/100
 - 89s - loss: 0.6722 - acc: 0.8441 - val_loss: 1.8341 - val_acc: 0.6870
Epoch 98/100
 - 89s - loss: 0.7128 - acc: 0.8342 - val_loss: 1.8301 - val_acc: 0.6770
Epoch 99/100
 - 89s - loss: 0.6848 - acc: 0.8411 - val_loss: 2.0956 - val_acc: 0.6753
Epoch 100/100
 - 89s - loss: 0.6891 - acc: 0.8399 - val_loss: 2.0825 - val_acc: 0.6828
Reached validation accuracy is 0.6828333338101705
8887.85925579071
