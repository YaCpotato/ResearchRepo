wrn_28_10 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 100)          64100       global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,564,932
Trainable params: 36,546,980
Non-trainable params: 17,952
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'additive-gaussian-noise', 'aug1_magnitude': 0.795, 'aug2_type': 'brighten', 'aug2_magnitude': 0.7120000000000001}, {'aug1_type': 'brighten', 'aug1_magnitude': 0.003, 'aug2_type': 'shear', 'aug2_magnitude': 0.012}, {'aug1_type': 'vertical-flip', 'aug1_magnitude': 0.7390000000000001, 'aug2_type': 'translate-x', 'aug2_magnitude': 0.322}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.20800000000000002, 'aug2_type': 'coarse-dropout', 'aug2_magnitude': 0.475}, {'aug1_type': 'translate-y', 'aug1_magnitude': 0.7809999999999999, 'aug2_type': 'coarse-dropout', 'aug2_magnitude': 0.789}]

 - 98s - loss: 4.4717 - acc: 0.0634 - val_loss: 4.5117 - val_acc: 0.0978
Epoch 2/100
 - 88s - loss: 3.9870 - acc: 0.1132 - val_loss: 3.5590 - val_acc: 0.1840
Epoch 3/100
 - 88s - loss: 3.6945 - acc: 0.1595 - val_loss: 3.3514 - val_acc: 0.2033
Epoch 4/100
 - 88s - loss: 3.4147 - acc: 0.2021 - val_loss: 2.8896 - val_acc: 0.2875
Epoch 5/100
 - 88s - loss: 3.1160 - acc: 0.2542 - val_loss: 2.7300 - val_acc: 0.3375
Epoch 6/100
 - 88s - loss: 2.8809 - acc: 0.3048 - val_loss: 2.9179 - val_acc: 0.3468
Epoch 7/100
 - 88s - loss: 2.7198 - acc: 0.3378 - val_loss: 2.6008 - val_acc: 0.3995
Epoch 8/100
 - 88s - loss: 2.5816 - acc: 0.3628 - val_loss: 2.1611 - val_acc: 0.4230
Epoch 9/100
 - 89s - loss: 2.4413 - acc: 0.3956 - val_loss: 2.1365 - val_acc: 0.4483
Epoch 10/100
 - 88s - loss: 2.3417 - acc: 0.4187 - val_loss: 1.9637 - val_acc: 0.4760
Epoch 11/100
 - 88s - loss: 2.2310 - acc: 0.4391 - val_loss: 1.9677 - val_acc: 0.4967
Epoch 12/100
 - 89s - loss: 2.0636 - acc: 0.4621 - val_loss: 1.8310 - val_acc: 0.5248
Epoch 13/100
 - 89s - loss: 1.9415 - acc: 0.4871 - val_loss: 1.6683 - val_acc: 0.5453
Epoch 14/100
 - 88s - loss: 1.8455 - acc: 0.5091 - val_loss: 1.9213 - val_acc: 0.5135
Epoch 15/100
 - 88s - loss: 1.7385 - acc: 0.5354 - val_loss: 1.5672 - val_acc: 0.5805
Epoch 16/100
 - 88s - loss: 1.6584 - acc: 0.5562 - val_loss: 1.6747 - val_acc: 0.5692
Epoch 17/100
 - 89s - loss: 1.5820 - acc: 0.5733 - val_loss: 1.6244 - val_acc: 0.5893
Epoch 18/100
 - 88s - loss: 1.5371 - acc: 0.5855 - val_loss: 1.9626 - val_acc: 0.5297
Epoch 19/100
 - 88s - loss: 1.4747 - acc: 0.6026 - val_loss: 1.4734 - val_acc: 0.6147
Epoch 20/100
 - 88s - loss: 1.4038 - acc: 0.6175 - val_loss: 1.5650 - val_acc: 0.6073
Epoch 21/100
 - 88s - loss: 1.3492 - acc: 0.6306 - val_loss: 1.5509 - val_acc: 0.6015
Epoch 22/100
 - 89s - loss: 1.2786 - acc: 0.6507 - val_loss: 1.4371 - val_acc: 0.6318
Epoch 23/100
 - 88s - loss: 1.2479 - acc: 0.6594 - val_loss: 1.4241 - val_acc: 0.6298
Epoch 24/100
 - 88s - loss: 1.1959 - acc: 0.6712 - val_loss: 1.3831 - val_acc: 0.6427
Epoch 25/100
 - 88s - loss: 1.1606 - acc: 0.6830 - val_loss: 1.4276 - val_acc: 0.6440
Epoch 26/100
 - 89s - loss: 1.1180 - acc: 0.6946 - val_loss: 1.3624 - val_acc: 0.6568
Epoch 27/100
 - 89s - loss: 1.0835 - acc: 0.7014 - val_loss: 1.4998 - val_acc: 0.6460
Epoch 28/100
 - 90s - loss: 1.0498 - acc: 0.7120 - val_loss: 1.6428 - val_acc: 0.6115
Epoch 29/100
 - 90s - loss: 0.9993 - acc: 0.7259 - val_loss: 1.6007 - val_acc: 0.6403
Epoch 30/100
 - 89s - loss: 0.9908 - acc: 0.7286 - val_loss: 1.6713 - val_acc: 0.6303
Epoch 31/100
 - 88s - loss: 0.9648 - acc: 0.7367 - val_loss: 1.5276 - val_acc: 0.6640
Epoch 32/100
 - 88s - loss: 0.9544 - acc: 0.7421 - val_loss: 1.5270 - val_acc: 0.6507
Epoch 33/100
 - 87s - loss: 0.9116 - acc: 0.7501 - val_loss: 1.4049 - val_acc: 0.6552
Epoch 34/100
 - 88s - loss: 0.9095 - acc: 0.7517 - val_loss: 1.6072 - val_acc: 0.6515
Epoch 35/100
 - 87s - loss: 0.8767 - acc: 0.7617 - val_loss: 1.5654 - val_acc: 0.6573
Epoch 36/100
 - 88s - loss: 0.8682 - acc: 0.7651 - val_loss: 1.4584 - val_acc: 0.6723
Epoch 37/100
 - 89s - loss: 0.8456 - acc: 0.7702 - val_loss: 1.8254 - val_acc: 0.6412
Epoch 38/100
 - 89s - loss: 0.8410 - acc: 0.7717 - val_loss: 1.5792 - val_acc: 0.6582
Epoch 39/100
 - 89s - loss: 0.8286 - acc: 0.7760 - val_loss: 1.5164 - val_acc: 0.6607
Epoch 40/100
 - 88s - loss: 0.8163 - acc: 0.7794 - val_loss: 1.6262 - val_acc: 0.6560
Epoch 41/100
 - 89s - loss: 0.8051 - acc: 0.7788 - val_loss: 1.6353 - val_acc: 0.6675
Epoch 42/100
 - 87s - loss: 0.7767 - acc: 0.7912 - val_loss: 1.6358 - val_acc: 0.6627
Epoch 43/100
 - 88s - loss: 0.7645 - acc: 0.7943 - val_loss: 1.5764 - val_acc: 0.6722
Epoch 44/100
 - 88s - loss: 0.7558 - acc: 0.7949 - val_loss: 1.4766 - val_acc: 0.6928
Epoch 45/100
 - 88s - loss: 0.7542 - acc: 0.7952 - val_loss: 1.7208 - val_acc: 0.6668
Epoch 46/100
 - 89s - loss: 0.7434 - acc: 0.7995 - val_loss: 1.5283 - val_acc: 0.6828
Epoch 47/100
 - 89s - loss: 0.7273 - acc: 0.8021 - val_loss: 1.6773 - val_acc: 0.6625
Epoch 48/100
 - 90s - loss: 0.7217 - acc: 0.8039 - val_loss: 1.6319 - val_acc: 0.6807
Epoch 49/100
 - 89s - loss: 0.7070 - acc: 0.8097 - val_loss: 1.5597 - val_acc: 0.6827
Epoch 50/100
 - 89s - loss: 0.6947 - acc: 0.8124 - val_loss: 1.6293 - val_acc: 0.6770
Epoch 51/100
 - 89s - loss: 0.6910 - acc: 0.8136 - val_loss: 1.5341 - val_acc: 0.6850
Epoch 52/100
 - 89s - loss: 0.6857 - acc: 0.8158 - val_loss: 1.7049 - val_acc: 0.6722
Epoch 53/100
 - 89s - loss: 0.6834 - acc: 0.8156 - val_loss: 1.5304 - val_acc: 0.6898
Epoch 54/100
 - 89s - loss: 0.6722 - acc: 0.8191 - val_loss: 1.6701 - val_acc: 0.6757
Epoch 55/100
 - 88s - loss: 0.6758 - acc: 0.8173 - val_loss: 1.8076 - val_acc: 0.6665
Epoch 56/100
 - 88s - loss: 0.6481 - acc: 0.8250 - val_loss: 1.6186 - val_acc: 0.6845
Epoch 57/100
 - 87s - loss: 0.6390 - acc: 0.8270 - val_loss: 1.5740 - val_acc: 0.6890
Epoch 58/100
 - 88s - loss: 0.6451 - acc: 0.8263 - val_loss: 1.8376 - val_acc: 0.6602
Epoch 59/100
 - 88s - loss: 0.6528 - acc: 0.8224 - val_loss: 1.6118 - val_acc: 0.6823
Epoch 60/100
 - 88s - loss: 0.6443 - acc: 0.8247 - val_loss: 1.5759 - val_acc: 0.6947
Epoch 61/100
 - 89s - loss: 0.6323 - acc: 0.8301 - val_loss: 1.5185 - val_acc: 0.6908
Epoch 62/100
 - 88s - loss: 0.6288 - acc: 0.8297 - val_loss: 1.7000 - val_acc: 0.6750
Epoch 63/100
 - 88s - loss: 0.6138 - acc: 0.8325 - val_loss: 1.7964 - val_acc: 0.6587
Epoch 64/100
 - 88s - loss: 0.6120 - acc: 0.8355 - val_loss: 1.6023 - val_acc: 0.6918
Epoch 65/100
 - 88s - loss: 0.5980 - acc: 0.8385 - val_loss: 1.7518 - val_acc: 0.6860
Epoch 66/100
 - 88s - loss: 0.5936 - acc: 0.8404 - val_loss: 1.5872 - val_acc: 0.6883
Epoch 67/100
 - 89s - loss: 0.5837 - acc: 0.8417 - val_loss: 1.5608 - val_acc: 0.6945
Epoch 68/100
 - 88s - loss: 0.5826 - acc: 0.8433 - val_loss: 1.7356 - val_acc: 0.6742
Epoch 69/100
 - 88s - loss: 0.5825 - acc: 0.8422 - val_loss: 1.7513 - val_acc: 0.6893
Epoch 70/100
 - 88s - loss: 0.5703 - acc: 0.8450 - val_loss: 1.9466 - val_acc: 0.6702
Epoch 71/100
 - 88s - loss: 0.5821 - acc: 0.8431 - val_loss: 1.5070 - val_acc: 0.7047
Epoch 72/100
 - 88s - loss: 0.5608 - acc: 0.8487 - val_loss: 1.7824 - val_acc: 0.6957
Epoch 73/100
 - 88s - loss: 0.5671 - acc: 0.8470 - val_loss: 1.8048 - val_acc: 0.6852
Epoch 74/100
 - 88s - loss: 0.5585 - acc: 0.8488 - val_loss: 1.6708 - val_acc: 0.6925
Epoch 75/100
 - 88s - loss: 0.5539 - acc: 0.8497 - val_loss: 1.7248 - val_acc: 0.6758
Epoch 76/100
 - 88s - loss: 0.5605 - acc: 0.8487 - val_loss: 1.7243 - val_acc: 0.6915
Epoch 77/100
 - 88s - loss: 0.5683 - acc: 0.8470 - val_loss: 1.7751 - val_acc: 0.6757
Epoch 78/100
 - 89s - loss: 0.5468 - acc: 0.8516 - val_loss: 1.7199 - val_acc: 0.6917
Epoch 79/100
 - 89s - loss: 0.5498 - acc: 0.8506 - val_loss: 1.5018 - val_acc: 0.7058
Epoch 80/100
 - 88s - loss: 0.5406 - acc: 0.8521 - val_loss: 1.5525 - val_acc: 0.7133
Epoch 81/100
 - 89s - loss: 0.5326 - acc: 0.8564 - val_loss: 1.6996 - val_acc: 0.6997
Epoch 82/100
 - 90s - loss: 0.5445 - acc: 0.8520 - val_loss: 1.6466 - val_acc: 0.6990
Epoch 83/100
 - 90s - loss: 0.5233 - acc: 0.8587 - val_loss: 1.7838 - val_acc: 0.6867
Epoch 84/100
 - 89s - loss: 0.5225 - acc: 0.8600 - val_loss: 1.6229 - val_acc: 0.6987
Epoch 85/100
 - 88s - loss: 0.5347 - acc: 0.8555 - val_loss: 1.6205 - val_acc: 0.7053
Epoch 86/100
 - 89s - loss: 0.5276 - acc: 0.8578 - val_loss: 1.6642 - val_acc: 0.6965
Epoch 87/100
 - 88s - loss: 0.5256 - acc: 0.8599 - val_loss: 1.6274 - val_acc: 0.7025
Epoch 88/100
 - 89s - loss: 0.5082 - acc: 0.8631 - val_loss: 1.9229 - val_acc: 0.6835
Epoch 89/100
 - 89s - loss: 0.5151 - acc: 0.8590 - val_loss: 1.5063 - val_acc: 0.7033
Epoch 90/100
 - 87s - loss: 0.5127 - acc: 0.8616 - val_loss: 1.7667 - val_acc: 0.6945
Epoch 91/100
 - 88s - loss: 0.4984 - acc: 0.8651 - val_loss: 1.6588 - val_acc: 0.6997
Epoch 92/100
 - 87s - loss: 0.5066 - acc: 0.8624 - val_loss: 1.7831 - val_acc: 0.6888
Epoch 93/100
 - 88s - loss: 0.5125 - acc: 0.8610 - val_loss: 1.8138 - val_acc: 0.6958
Epoch 94/100
 - 87s - loss: 0.4990 - acc: 0.8635 - val_loss: 1.7839 - val_acc: 0.6850
Epoch 95/100
 - 88s - loss: 0.5036 - acc: 0.8634 - val_loss: 1.7029 - val_acc: 0.6988
Epoch 96/100
 - 87s - loss: 0.4902 - acc: 0.8677 - val_loss: 1.6086 - val_acc: 0.7052
Epoch 97/100
 - 88s - loss: 0.4787 - acc: 0.8713 - val_loss: 1.7922 - val_acc: 0.6942
Epoch 98/100
 - 87s - loss: 0.4822 - acc: 0.8706 - val_loss: 1.5964 - val_acc: 0.6972
Epoch 99/100
 - 88s - loss: 0.4773 - acc: 0.8704 - val_loss: 1.6485 - val_acc: 0.7090
Epoch 100/100
 - 88s - loss: 0.4904 - acc: 0.8670 - val_loss: 1.6406 - val_acc: 0.6913
Reached validation accuracy is 0.6913333336512247
8850.70109987259
