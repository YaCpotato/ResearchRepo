wrn_28_10 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           6410        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,507,242
Trainable params: 36,489,290
Non-trainable params: 17,952
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'translate-x', 'aug1_magnitude': 0.838, 'aug2_type': 'shear', 'aug2_magnitude': 0.392}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.154, 'aug2_type': 'shear', 'aug2_magnitude': 0.34700000000000003}, {'aug1_type': 'dropout', 'aug1_magnitude': 0.077, 'aug2_type': 'horizontal-flip', 'aug2_magnitude': 0.635}, {'aug1_type': 'gamma-contrast', 'aug1_magnitude': 0.758, 'aug2_type': 'translate-y', 'aug2_magnitude': 0.9079999999999999}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.8809999999999999, 'aug2_type': 'RandomEracing', 'aug2_magnitude': 0.467}]

 - 99s - loss: 2.3242 - acc: 0.3024 - val_loss: 2.4838 - val_acc: 0.2773
Epoch 2/100
 - 89s - loss: 1.5648 - acc: 0.4597 - val_loss: 1.7860 - val_acc: 0.4585
Epoch 3/100
 - 89s - loss: 1.3529 - acc: 0.5406 - val_loss: 1.4844 - val_acc: 0.5222
Epoch 4/100
 - 86s - loss: 1.2049 - acc: 0.5967 - val_loss: 1.2727 - val_acc: 0.6062
Epoch 5/100
 - 86s - loss: 1.0883 - acc: 0.6381 - val_loss: 0.9762 - val_acc: 0.6947
Epoch 6/100
 - 88s - loss: 0.9591 - acc: 0.6736 - val_loss: 1.1136 - val_acc: 0.6802
Epoch 7/100
 - 87s - loss: 0.8563 - acc: 0.7053 - val_loss: 0.7725 - val_acc: 0.7343
Epoch 8/100
 - 88s - loss: 0.7839 - acc: 0.7267 - val_loss: 0.6727 - val_acc: 0.7820
Epoch 9/100
 - 88s - loss: 0.7356 - acc: 0.7424 - val_loss: 0.9989 - val_acc: 0.6942
Epoch 10/100
 - 88s - loss: 0.6871 - acc: 0.7592 - val_loss: 0.7555 - val_acc: 0.7602
Epoch 11/100
 - 88s - loss: 0.6492 - acc: 0.7713 - val_loss: 0.5486 - val_acc: 0.8143
Epoch 12/100
 - 88s - loss: 0.6123 - acc: 0.7856 - val_loss: 0.5203 - val_acc: 0.8277
Epoch 13/100
 - 89s - loss: 0.5771 - acc: 0.7980 - val_loss: 0.4621 - val_acc: 0.8502
Epoch 14/100
 - 89s - loss: 0.5548 - acc: 0.8069 - val_loss: 0.6472 - val_acc: 0.7987
Epoch 15/100
 - 88s - loss: 0.5350 - acc: 0.8123 - val_loss: 0.3739 - val_acc: 0.8715
Epoch 16/100
 - 88s - loss: 0.5089 - acc: 0.8217 - val_loss: 0.5266 - val_acc: 0.8333
Epoch 17/100
 - 88s - loss: 0.4863 - acc: 0.8288 - val_loss: 0.7607 - val_acc: 0.7692
Epoch 18/100
 - 88s - loss: 0.4738 - acc: 0.8327 - val_loss: 0.4246 - val_acc: 0.8590
Epoch 19/100
 - 88s - loss: 0.4532 - acc: 0.8392 - val_loss: 0.4129 - val_acc: 0.8642
Epoch 20/100
 - 88s - loss: 0.4349 - acc: 0.8469 - val_loss: 0.3980 - val_acc: 0.8740
Epoch 21/100
 - 88s - loss: 0.4269 - acc: 0.8492 - val_loss: 0.3751 - val_acc: 0.8802
Epoch 22/100
 - 89s - loss: 0.4009 - acc: 0.8582 - val_loss: 0.3846 - val_acc: 0.8812
Epoch 23/100
 - 90s - loss: 0.4022 - acc: 0.8582 - val_loss: 0.4531 - val_acc: 0.8597
Epoch 24/100
 - 90s - loss: 0.3865 - acc: 0.8628 - val_loss: 0.4369 - val_acc: 0.8678
Epoch 25/100
 - 89s - loss: 0.3801 - acc: 0.8665 - val_loss: 0.4065 - val_acc: 0.8833
Epoch 26/100
 - 89s - loss: 0.3649 - acc: 0.8720 - val_loss: 0.4245 - val_acc: 0.8758
Epoch 27/100
 - 90s - loss: 0.3583 - acc: 0.8737 - val_loss: 0.4368 - val_acc: 0.8782
Epoch 28/100
 - 89s - loss: 0.3519 - acc: 0.8758 - val_loss: 0.3516 - val_acc: 0.8905
Epoch 29/100
 - 89s - loss: 0.3419 - acc: 0.8800 - val_loss: 0.3574 - val_acc: 0.8983
Epoch 30/100
 - 88s - loss: 0.3317 - acc: 0.8823 - val_loss: 0.3675 - val_acc: 0.8987
Epoch 31/100
 - 88s - loss: 0.3278 - acc: 0.8843 - val_loss: 0.3383 - val_acc: 0.9015
Epoch 32/100
 - 89s - loss: 0.3190 - acc: 0.8887 - val_loss: 0.4219 - val_acc: 0.8785
Epoch 33/100
 - 89s - loss: 0.3126 - acc: 0.8911 - val_loss: 0.3058 - val_acc: 0.9053
Epoch 34/100
 - 88s - loss: 0.3168 - acc: 0.8899 - val_loss: 0.4459 - val_acc: 0.8897
Epoch 35/100
 - 88s - loss: 0.3068 - acc: 0.8914 - val_loss: 0.3340 - val_acc: 0.9042
Epoch 36/100
 - 88s - loss: 0.2989 - acc: 0.8931 - val_loss: 0.3567 - val_acc: 0.8987
Epoch 37/100
 - 88s - loss: 0.2963 - acc: 0.8954 - val_loss: 0.3056 - val_acc: 0.9135
Epoch 38/100
 - 88s - loss: 0.2936 - acc: 0.8968 - val_loss: 0.3407 - val_acc: 0.9010
Epoch 39/100
 - 88s - loss: 0.2890 - acc: 0.8986 - val_loss: 0.2756 - val_acc: 0.9207
Epoch 40/100
 - 88s - loss: 0.2796 - acc: 0.9018 - val_loss: 0.4209 - val_acc: 0.8907
Epoch 41/100
 - 89s - loss: 0.2837 - acc: 0.8999 - val_loss: 0.3421 - val_acc: 0.9125
Epoch 42/100
 - 89s - loss: 0.2713 - acc: 0.9048 - val_loss: 0.3257 - val_acc: 0.9127
Epoch 43/100
 - 89s - loss: 0.2675 - acc: 0.9054 - val_loss: 0.3771 - val_acc: 0.8988
Epoch 44/100
 - 89s - loss: 0.2658 - acc: 0.9067 - val_loss: 0.2783 - val_acc: 0.9228
Epoch 45/100
 - 88s - loss: 0.2676 - acc: 0.9075 - val_loss: 0.3556 - val_acc: 0.9142
Epoch 46/100
 - 89s - loss: 0.2627 - acc: 0.9081 - val_loss: 0.3284 - val_acc: 0.9110
Epoch 47/100
 - 88s - loss: 0.2572 - acc: 0.9099 - val_loss: 0.3135 - val_acc: 0.9120
Epoch 48/100
 - 88s - loss: 0.2618 - acc: 0.9071 - val_loss: 0.3525 - val_acc: 0.9112
Epoch 49/100
 - 88s - loss: 0.2495 - acc: 0.9125 - val_loss: 0.4109 - val_acc: 0.8968
Epoch 50/100
 - 88s - loss: 0.2608 - acc: 0.9079 - val_loss: 0.3269 - val_acc: 0.9162
Epoch 51/100
 - 87s - loss: 0.2487 - acc: 0.9138 - val_loss: 0.2780 - val_acc: 0.9262
Epoch 52/100
 - 88s - loss: 0.2472 - acc: 0.9122 - val_loss: 0.4432 - val_acc: 0.9005
Epoch 53/100
 - 87s - loss: 0.2402 - acc: 0.9164 - val_loss: 0.2726 - val_acc: 0.9278
Epoch 54/100
 - 88s - loss: 0.2437 - acc: 0.9134 - val_loss: 0.3629 - val_acc: 0.9135
Epoch 55/100
 - 88s - loss: 0.2435 - acc: 0.9148 - val_loss: 0.2840 - val_acc: 0.9260
Epoch 56/100
 - 87s - loss: 0.2363 - acc: 0.9161 - val_loss: 0.2771 - val_acc: 0.9243
Epoch 57/100
 - 89s - loss: 0.2319 - acc: 0.9171 - val_loss: 0.2929 - val_acc: 0.9225
Epoch 58/100
 - 89s - loss: 0.2351 - acc: 0.9164 - val_loss: 0.2937 - val_acc: 0.9263
Epoch 59/100
 - 89s - loss: 0.2282 - acc: 0.9199 - val_loss: 0.2982 - val_acc: 0.9208
Epoch 60/100
 - 89s - loss: 0.2267 - acc: 0.9200 - val_loss: 0.4123 - val_acc: 0.9038
Epoch 61/100
 - 89s - loss: 0.2272 - acc: 0.9199 - val_loss: 0.3394 - val_acc: 0.9198
Epoch 62/100
 - 87s - loss: 0.2218 - acc: 0.9220 - val_loss: 0.3809 - val_acc: 0.9085
Epoch 63/100
 - 88s - loss: 0.2246 - acc: 0.9212 - val_loss: 0.3014 - val_acc: 0.9247
Epoch 64/100
 - 88s - loss: 0.2251 - acc: 0.9215 - val_loss: 0.3066 - val_acc: 0.9277
Epoch 65/100
 - 87s - loss: 0.2183 - acc: 0.9243 - val_loss: 0.3366 - val_acc: 0.9178
Epoch 66/100
 - 87s - loss: 0.2195 - acc: 0.9221 - val_loss: 0.3013 - val_acc: 0.9238
Epoch 67/100
 - 88s - loss: 0.2218 - acc: 0.9224 - val_loss: 0.3407 - val_acc: 0.9235
Epoch 68/100
 - 87s - loss: 0.2141 - acc: 0.9246 - val_loss: 0.3098 - val_acc: 0.9298
Epoch 69/100
 - 88s - loss: 0.2131 - acc: 0.9252 - val_loss: 0.2610 - val_acc: 0.9292
Epoch 70/100
 - 87s - loss: 0.2141 - acc: 0.9250 - val_loss: 0.2963 - val_acc: 0.9320
Epoch 71/100
 - 85s - loss: 0.2131 - acc: 0.9245 - val_loss: 0.3136 - val_acc: 0.9195
Epoch 72/100
 - 84s - loss: 0.2052 - acc: 0.9278 - val_loss: 0.3177 - val_acc: 0.9212
Epoch 73/100
 - 83s - loss: 0.2108 - acc: 0.9255 - val_loss: 0.3360 - val_acc: 0.9233
Epoch 74/100
 - 86s - loss: 0.2146 - acc: 0.9242 - val_loss: 0.2947 - val_acc: 0.9295
Epoch 75/100
 - 87s - loss: 0.2101 - acc: 0.9271 - val_loss: 0.3128 - val_acc: 0.9285
Epoch 76/100
 - 87s - loss: 0.2037 - acc: 0.9286 - val_loss: 0.3215 - val_acc: 0.9270
Epoch 77/100
 - 86s - loss: 0.2095 - acc: 0.9267 - val_loss: 0.4423 - val_acc: 0.9145
Epoch 78/100
 - 87s - loss: 0.2077 - acc: 0.9272 - val_loss: 0.3022 - val_acc: 0.9287
Epoch 79/100
 - 85s - loss: 0.2038 - acc: 0.9287 - val_loss: 0.2623 - val_acc: 0.9332
Epoch 80/100
 - 86s - loss: 0.1978 - acc: 0.9310 - val_loss: 0.2902 - val_acc: 0.9255
Epoch 81/100
 - 87s - loss: 0.2058 - acc: 0.9278 - val_loss: 0.2673 - val_acc: 0.9317
Epoch 82/100
 - 85s - loss: 0.2040 - acc: 0.9282 - val_loss: 0.4345 - val_acc: 0.9090
Epoch 83/100
 - 87s - loss: 0.1963 - acc: 0.9315 - val_loss: 0.2671 - val_acc: 0.9335
Epoch 84/100
 - 87s - loss: 0.1946 - acc: 0.9311 - val_loss: 0.3287 - val_acc: 0.9270
Epoch 85/100
 - 87s - loss: 0.1999 - acc: 0.9295 - val_loss: 0.2911 - val_acc: 0.9332
Epoch 86/100
 - 89s - loss: 0.1977 - acc: 0.9312 - val_loss: 0.3186 - val_acc: 0.9312
Epoch 87/100
 - 89s - loss: 0.1910 - acc: 0.9329 - val_loss: 0.3443 - val_acc: 0.9242
Epoch 88/100
 - 89s - loss: 0.1955 - acc: 0.9309 - val_loss: 0.3091 - val_acc: 0.9320
Epoch 89/100
 - 88s - loss: 0.1959 - acc: 0.9309 - val_loss: 0.2696 - val_acc: 0.9372
Epoch 90/100
 - 86s - loss: 0.1959 - acc: 0.9300 - val_loss: 0.3401 - val_acc: 0.9240
Epoch 91/100
 - 87s - loss: 0.1973 - acc: 0.9304 - val_loss: 0.3377 - val_acc: 0.9243
Epoch 92/100
 - 87s - loss: 0.1921 - acc: 0.9324 - val_loss: 0.3159 - val_acc: 0.9272
Epoch 93/100
 - 87s - loss: 0.1864 - acc: 0.9319 - val_loss: 0.2648 - val_acc: 0.9375
Epoch 94/100
 - 86s - loss: 0.1886 - acc: 0.9327 - val_loss: 0.2976 - val_acc: 0.9350
Epoch 95/100
 - 87s - loss: 0.1908 - acc: 0.9319 - val_loss: 0.3872 - val_acc: 0.9143
Epoch 96/100
 - 87s - loss: 0.1855 - acc: 0.9350 - val_loss: 0.2707 - val_acc: 0.9360
Epoch 97/100
 - 87s - loss: 0.1877 - acc: 0.9333 - val_loss: 0.2969 - val_acc: 0.9357
Epoch 98/100
 - 87s - loss: 0.1879 - acc: 0.9338 - val_loss: 0.2507 - val_acc: 0.9410
Epoch 99/100
 - 86s - loss: 0.1840 - acc: 0.9349 - val_loss: 0.2691 - val_acc: 0.9355
Epoch 100/100
 - 85s - loss: 0.1826 - acc: 0.9350 - val_loss: 0.3958 - val_acc: 0.9248
Reached validation accuracy is 0.9248333330154419
8791.831767082214
