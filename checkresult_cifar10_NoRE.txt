wrn_28_10 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           6410        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,507,242
Trainable params: 36,489,290
Non-trainable params: 17,952
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'additive-gaussian-noise', 'aug1_magnitude': 0.022000000000000002, 'aug2_type': 'gamma-contrast', 'aug2_magnitude': 0.927}, {'aug1_type': 'coarse-dropout', 'aug1_magnitude': 0.042, 'aug2_type': 'brighten', 'aug2_magnitude': 0.128}, {'aug1_type': 'brighten', 'aug1_magnitude': 0.363, 'aug2_type': 'gamma-contrast', 'aug2_magnitude': 0.747}, {'aug1_type': 'crop', 'aug1_magnitude': 0.7340000000000001, 'aug2_type': 'shear', 'aug2_magnitude': 0.201}, {'aug1_type': 'crop', 'aug1_magnitude': 0.212, 'aug2_type': 'invert', 'aug2_magnitude': 0.878}]

 - 98s - loss: 2.5899 - acc: 0.2480 - val_loss: 1.6407 - val_acc: 0.3890
Epoch 2/100
 - 87s - loss: 1.8392 - acc: 0.3890 - val_loss: 1.5319 - val_acc: 0.4528
Epoch 3/100
 - 88s - loss: 1.5382 - acc: 0.4744 - val_loss: 1.1858 - val_acc: 0.5863
Epoch 4/100
 - 87s - loss: 1.2360 - acc: 0.5621 - val_loss: 1.0856 - val_acc: 0.6635
Epoch 5/100
 - 88s - loss: 1.0755 - acc: 0.6210 - val_loss: 0.9480 - val_acc: 0.6810
Epoch 6/100
 - 87s - loss: 0.9659 - acc: 0.6603 - val_loss: 0.8935 - val_acc: 0.7005
Epoch 7/100
 - 87s - loss: 0.8841 - acc: 0.6869 - val_loss: 0.8590 - val_acc: 0.6975
Epoch 8/100
 - 88s - loss: 0.8286 - acc: 0.7077 - val_loss: 0.8647 - val_acc: 0.7125
Epoch 9/100
 - 87s - loss: 0.7845 - acc: 0.7218 - val_loss: 0.7240 - val_acc: 0.7573
Epoch 10/100
 - 88s - loss: 0.7530 - acc: 0.7340 - val_loss: 0.5609 - val_acc: 0.8112
Epoch 11/100
 - 89s - loss: 0.7118 - acc: 0.7470 - val_loss: 0.5720 - val_acc: 0.8133
Epoch 12/100
 - 89s - loss: 0.6678 - acc: 0.7601 - val_loss: 0.5133 - val_acc: 0.8307
Epoch 13/100
 - 89s - loss: 0.6505 - acc: 0.7680 - val_loss: 0.5247 - val_acc: 0.8422
Epoch 14/100
 - 88s - loss: 0.6219 - acc: 0.7782 - val_loss: 0.6199 - val_acc: 0.8117
Epoch 15/100
 - 88s - loss: 0.5940 - acc: 0.7871 - val_loss: 0.5730 - val_acc: 0.8208
Epoch 16/100
 - 89s - loss: 0.5782 - acc: 0.7929 - val_loss: 0.5946 - val_acc: 0.8158
Epoch 17/100
 - 87s - loss: 0.5628 - acc: 0.7995 - val_loss: 0.4623 - val_acc: 0.8583
Epoch 18/100
 - 88s - loss: 0.5426 - acc: 0.8065 - val_loss: 0.4615 - val_acc: 0.8578
Epoch 19/100
 - 88s - loss: 0.5193 - acc: 0.8145 - val_loss: 0.5859 - val_acc: 0.8327
Epoch 20/100
 - 86s - loss: 0.5119 - acc: 0.8168 - val_loss: 0.4783 - val_acc: 0.8588
Epoch 21/100
 - 87s - loss: 0.4965 - acc: 0.8228 - val_loss: 0.5747 - val_acc: 0.8300
Epoch 22/100
 - 88s - loss: 0.4870 - acc: 0.8245 - val_loss: 0.5287 - val_acc: 0.8480
Epoch 23/100
 - 88s - loss: 0.4728 - acc: 0.8311 - val_loss: 0.4159 - val_acc: 0.8675
Epoch 24/100
 - 86s - loss: 0.4627 - acc: 0.8353 - val_loss: 0.3476 - val_acc: 0.8942
Epoch 25/100
 - 87s - loss: 0.4475 - acc: 0.8401 - val_loss: 0.4972 - val_acc: 0.8613
Epoch 26/100
 - 88s - loss: 0.4584 - acc: 0.8349 - val_loss: 0.5843 - val_acc: 0.8418
Epoch 27/100
 - 87s - loss: 0.4441 - acc: 0.8414 - val_loss: 0.4032 - val_acc: 0.8892
Epoch 28/100
 - 88s - loss: 0.4327 - acc: 0.8461 - val_loss: 0.4285 - val_acc: 0.8707
Epoch 29/100
 - 87s - loss: 0.4146 - acc: 0.8506 - val_loss: 0.4250 - val_acc: 0.8830
Epoch 30/100
 - 88s - loss: 0.4255 - acc: 0.8466 - val_loss: 0.3533 - val_acc: 0.8962
Epoch 31/100
 - 88s - loss: 0.4130 - acc: 0.8512 - val_loss: 0.3499 - val_acc: 0.9037
Epoch 32/100
 - 89s - loss: 0.4119 - acc: 0.8531 - val_loss: 0.4390 - val_acc: 0.8810
Epoch 33/100
 - 88s - loss: 0.4038 - acc: 0.8548 - val_loss: 0.4190 - val_acc: 0.8938
Epoch 34/100
 - 88s - loss: 0.4008 - acc: 0.8569 - val_loss: 0.4043 - val_acc: 0.8863
Epoch 35/100
 - 87s - loss: 0.3984 - acc: 0.8563 - val_loss: 0.3314 - val_acc: 0.9065
Epoch 36/100
 - 86s - loss: 0.3926 - acc: 0.8593 - val_loss: 0.4604 - val_acc: 0.8960
Epoch 37/100
 - 87s - loss: 0.3950 - acc: 0.8590 - val_loss: 0.4458 - val_acc: 0.8978
Epoch 38/100
 - 88s - loss: 0.3858 - acc: 0.8623 - val_loss: 0.4288 - val_acc: 0.8878
Epoch 39/100
 - 89s - loss: 0.3850 - acc: 0.8602 - val_loss: 0.4667 - val_acc: 0.8853
Epoch 40/100
 - 88s - loss: 0.3883 - acc: 0.8609 - val_loss: 0.3568 - val_acc: 0.9097
Epoch 41/100
 - 88s - loss: 0.3739 - acc: 0.8661 - val_loss: 0.3422 - val_acc: 0.9110
Epoch 42/100
 - 88s - loss: 0.3628 - acc: 0.8687 - val_loss: 0.3635 - val_acc: 0.9012
Epoch 43/100
 - 88s - loss: 0.3744 - acc: 0.8643 - val_loss: 0.3336 - val_acc: 0.9073
Epoch 44/100
 - 89s - loss: 0.3651 - acc: 0.8687 - val_loss: 0.4267 - val_acc: 0.8982
Epoch 45/100
 - 88s - loss: 0.3645 - acc: 0.8688 - val_loss: 0.4428 - val_acc: 0.8838
Epoch 46/100
 - 88s - loss: 0.3580 - acc: 0.8717 - val_loss: 0.4186 - val_acc: 0.8990
Epoch 47/100
 - 89s - loss: 0.3670 - acc: 0.8671 - val_loss: 0.3470 - val_acc: 0.9102
Epoch 48/100
 - 88s - loss: 0.3567 - acc: 0.8701 - val_loss: 0.4165 - val_acc: 0.9055
Epoch 49/100
 - 87s - loss: 0.3472 - acc: 0.8753 - val_loss: 0.3535 - val_acc: 0.9165
Epoch 50/100
 - 88s - loss: 0.3571 - acc: 0.8719 - val_loss: 0.4352 - val_acc: 0.8963
Epoch 51/100
 - 88s - loss: 0.3442 - acc: 0.8766 - val_loss: 0.5079 - val_acc: 0.8938
Epoch 52/100
 - 88s - loss: 0.3510 - acc: 0.8735 - val_loss: 0.3959 - val_acc: 0.9148
Epoch 53/100
 - 88s - loss: 0.3411 - acc: 0.8754 - val_loss: 0.4727 - val_acc: 0.8800
Epoch 54/100
 - 89s - loss: 0.3420 - acc: 0.8770 - val_loss: 0.3471 - val_acc: 0.9158
Epoch 55/100
 - 88s - loss: 0.3328 - acc: 0.8790 - val_loss: 0.4679 - val_acc: 0.8905
Epoch 56/100
 - 88s - loss: 0.3495 - acc: 0.8751 - val_loss: 0.3821 - val_acc: 0.9070
Epoch 57/100
 - 87s - loss: 0.3308 - acc: 0.8802 - val_loss: 0.4372 - val_acc: 0.8987
Epoch 58/100
 - 88s - loss: 0.3481 - acc: 0.8746 - val_loss: 0.4580 - val_acc: 0.8963
Epoch 59/100
 - 89s - loss: 0.3265 - acc: 0.8818 - val_loss: 0.4680 - val_acc: 0.8868
Epoch 60/100
 - 88s - loss: 0.3258 - acc: 0.8828 - val_loss: 0.4628 - val_acc: 0.8983
Epoch 61/100
 - 89s - loss: 0.3306 - acc: 0.8813 - val_loss: 0.3404 - val_acc: 0.9130
Epoch 62/100
 - 89s - loss: 0.3342 - acc: 0.8807 - val_loss: 0.4574 - val_acc: 0.8987
Epoch 63/100
 - 89s - loss: 0.3159 - acc: 0.8872 - val_loss: 0.3445 - val_acc: 0.9220
Epoch 64/100
 - 89s - loss: 0.3268 - acc: 0.8832 - val_loss: 0.3963 - val_acc: 0.9102
Epoch 65/100
 - 89s - loss: 0.3235 - acc: 0.8838 - val_loss: 0.3471 - val_acc: 0.9182
Epoch 66/100
 - 88s - loss: 0.3221 - acc: 0.8830 - val_loss: 0.4135 - val_acc: 0.8993
Epoch 67/100
 - 88s - loss: 0.3223 - acc: 0.8842 - val_loss: 0.3979 - val_acc: 0.9057
Epoch 68/100
 - 88s - loss: 0.3135 - acc: 0.8858 - val_loss: 0.4030 - val_acc: 0.9050
Epoch 69/100
 - 89s - loss: 0.3240 - acc: 0.8843 - val_loss: 0.3639 - val_acc: 0.9115
Epoch 70/100
 - 88s - loss: 0.3298 - acc: 0.8815 - val_loss: 0.3501 - val_acc: 0.9163
Epoch 71/100
 - 89s - loss: 0.3215 - acc: 0.8852 - val_loss: 0.4181 - val_acc: 0.9023
Epoch 72/100
 - 88s - loss: 0.3180 - acc: 0.8854 - val_loss: 0.4124 - val_acc: 0.9105
Epoch 73/100
 - 88s - loss: 0.3155 - acc: 0.8864 - val_loss: 0.3678 - val_acc: 0.9193
Epoch 74/100
 - 88s - loss: 0.3228 - acc: 0.8840 - val_loss: 0.4585 - val_acc: 0.9030
Epoch 75/100
 - 88s - loss: 0.3026 - acc: 0.8922 - val_loss: 0.3176 - val_acc: 0.9200
Epoch 76/100
 - 88s - loss: 0.3141 - acc: 0.8869 - val_loss: 0.3295 - val_acc: 0.9213
Epoch 77/100
 - 88s - loss: 0.3149 - acc: 0.8865 - val_loss: 0.4406 - val_acc: 0.9062
Epoch 78/100
 - 89s - loss: 0.3089 - acc: 0.8881 - val_loss: 0.4386 - val_acc: 0.9073
Epoch 79/100
 - 89s - loss: 0.3177 - acc: 0.8848 - val_loss: 0.3987 - val_acc: 0.9118
Epoch 80/100
 - 88s - loss: 0.3040 - acc: 0.8907 - val_loss: 0.4068 - val_acc: 0.9102
Epoch 81/100
 - 88s - loss: 0.3066 - acc: 0.8894 - val_loss: 0.3332 - val_acc: 0.9230
Epoch 82/100
 - 89s - loss: 0.3094 - acc: 0.8880 - val_loss: 0.3431 - val_acc: 0.9235
Epoch 83/100
 - 88s - loss: 0.3085 - acc: 0.8883 - val_loss: 0.3553 - val_acc: 0.9173
Epoch 84/100
 - 88s - loss: 0.3201 - acc: 0.8859 - val_loss: 0.3646 - val_acc: 0.9128
Epoch 85/100
 - 88s - loss: 0.3059 - acc: 0.8903 - val_loss: 0.4130 - val_acc: 0.9127
Epoch 86/100
 - 88s - loss: 0.2963 - acc: 0.8924 - val_loss: 0.3351 - val_acc: 0.9265
Epoch 87/100
 - 88s - loss: 0.3106 - acc: 0.8885 - val_loss: 0.4022 - val_acc: 0.9162
Epoch 88/100
 - 88s - loss: 0.3035 - acc: 0.8920 - val_loss: 0.3347 - val_acc: 0.9203
Epoch 89/100
 - 89s - loss: 0.3037 - acc: 0.8911 - val_loss: 0.3690 - val_acc: 0.9178
Epoch 90/100
 - 89s - loss: 0.3076 - acc: 0.8881 - val_loss: 0.3798 - val_acc: 0.9187
Epoch 91/100
 - 89s - loss: 0.2963 - acc: 0.8921 - val_loss: 0.3830 - val_acc: 0.9132
Epoch 92/100
 - 89s - loss: 0.3022 - acc: 0.8918 - val_loss: 0.3557 - val_acc: 0.9152
Epoch 93/100
 - 89s - loss: 0.2973 - acc: 0.8927 - val_loss: 0.3396 - val_acc: 0.9215
Epoch 94/100
 - 89s - loss: 0.2992 - acc: 0.8926 - val_loss: 0.3468 - val_acc: 0.9243
Epoch 95/100
 - 89s - loss: 0.2998 - acc: 0.8922 - val_loss: 0.4051 - val_acc: 0.9055
Epoch 96/100
 - 89s - loss: 0.2973 - acc: 0.8924 - val_loss: 0.4919 - val_acc: 0.8990
Epoch 97/100
 - 89s - loss: 0.2975 - acc: 0.8924 - val_loss: 0.3791 - val_acc: 0.9117
Epoch 98/100
 - 89s - loss: 0.2982 - acc: 0.8925 - val_loss: 0.3829 - val_acc: 0.9177
Epoch 99/100
 - 89s - loss: 0.2926 - acc: 0.8950 - val_loss: 0.4296 - val_acc: 0.9070
Epoch 100/100
 - 89s - loss: 0.2958 - acc: 0.8935 - val_loss: 0.3793 - val_acc: 0.9165
Reached validation accuracy is 0.9165
8832.249443292618
