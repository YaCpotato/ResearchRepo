wrn_28_10 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 100)          64100       global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,564,932
Trainable params: 36,546,980
Non-trainable params: 17,952
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'sharpen', 'aug1_magnitude': 0.349, 'aug2_type': 'horizontal-flip', 'aug2_magnitude': 0.813}, {'aug1_type': 'RandomEracing', 'aug1_magnitude': 0.33399999999999996, 'aug2_type': 'brighten', 'aug2_magnitude': 0.603}, {'aug1_type': 'coarse-dropout', 'aug1_magnitude': 0.064, 'aug2_type': 'coarse-salt-pepper', 'aug2_magnitude': 0.8170000000000001}, {'aug1_type': 'vertical-flip', 'aug1_magnitude': 0.049, 'aug2_type': 'dropout', 'aug2_magnitude': 0.67}, {'aug1_type': 'crop', 'aug1_magnitude': 0.5379999999999999, 'aug2_type': 'brighten', 'aug2_magnitude': 0.141}]

 - 94s - loss: 4.3910 - acc: 0.0688 - val_loss: 4.0998 - val_acc: 0.1133
Epoch 2/100
 - 84s - loss: 3.9341 - acc: 0.1196 - val_loss: 3.8076 - val_acc: 0.1553
Epoch 3/100
 - 84s - loss: 3.6573 - acc: 0.1649 - val_loss: 3.3078 - val_acc: 0.2258
Epoch 4/100
 - 84s - loss: 3.4072 - acc: 0.2067 - val_loss: 3.1824 - val_acc: 0.2353
Epoch 5/100
 - 84s - loss: 3.1872 - acc: 0.2480 - val_loss: 2.7981 - val_acc: 0.3140
Epoch 6/100
 - 84s - loss: 2.9944 - acc: 0.2843 - val_loss: 2.7071 - val_acc: 0.3565
Epoch 7/100
 - 84s - loss: 2.8150 - acc: 0.3203 - val_loss: 2.3470 - val_acc: 0.3947
Epoch 8/100
 - 84s - loss: 2.6299 - acc: 0.3564 - val_loss: 2.1617 - val_acc: 0.4440
Epoch 9/100
 - 84s - loss: 2.5263 - acc: 0.3816 - val_loss: 2.1379 - val_acc: 0.4433
Epoch 10/100
 - 84s - loss: 2.3892 - acc: 0.4100 - val_loss: 2.0155 - val_acc: 0.4680
Epoch 11/100
 - 84s - loss: 2.2999 - acc: 0.4287 - val_loss: 1.9512 - val_acc: 0.4885
Epoch 12/100
 - 84s - loss: 2.1816 - acc: 0.4525 - val_loss: 1.9816 - val_acc: 0.4890
Epoch 13/100
 - 84s - loss: 2.1029 - acc: 0.4767 - val_loss: 1.8045 - val_acc: 0.5157
Epoch 14/100
 - 84s - loss: 1.9651 - acc: 0.4932 - val_loss: 1.6251 - val_acc: 0.5583
Epoch 15/100
 - 84s - loss: 1.8239 - acc: 0.5155 - val_loss: 1.6278 - val_acc: 0.5657
Epoch 16/100
 - 84s - loss: 1.7117 - acc: 0.5444 - val_loss: 1.6234 - val_acc: 0.5540
Epoch 17/100
 - 84s - loss: 1.6442 - acc: 0.5572 - val_loss: 1.5501 - val_acc: 0.5715
Epoch 18/100
 - 84s - loss: 1.5695 - acc: 0.5783 - val_loss: 1.6060 - val_acc: 0.5742
Epoch 19/100
 - 84s - loss: 1.4896 - acc: 0.5969 - val_loss: 2.7189 - val_acc: 0.5315
Epoch 20/100
 - 84s - loss: 1.4390 - acc: 0.6078 - val_loss: 1.4708 - val_acc: 0.6105
Epoch 21/100
 - 84s - loss: 1.3753 - acc: 0.6249 - val_loss: 1.4855 - val_acc: 0.6048
Epoch 22/100
 - 84s - loss: 1.3105 - acc: 0.6390 - val_loss: 2.0345 - val_acc: 0.5652
Epoch 23/100
 - 84s - loss: 1.2570 - acc: 0.6546 - val_loss: 1.6279 - val_acc: 0.6038
Epoch 24/100
 - 84s - loss: 1.2248 - acc: 0.6616 - val_loss: 1.3753 - val_acc: 0.6330
Epoch 25/100
 - 84s - loss: 1.1519 - acc: 0.6796 - val_loss: 1.3389 - val_acc: 0.6483
Epoch 26/100
 - 84s - loss: 1.1130 - acc: 0.6934 - val_loss: 1.8373 - val_acc: 0.6007
Epoch 27/100
 - 84s - loss: 1.0667 - acc: 0.7054 - val_loss: 1.5413 - val_acc: 0.6220
Epoch 28/100
 - 84s - loss: 1.0459 - acc: 0.7125 - val_loss: 1.4112 - val_acc: 0.6407
Epoch 29/100
 - 84s - loss: 1.0034 - acc: 0.7235 - val_loss: 1.4402 - val_acc: 0.6483
Epoch 30/100
 - 84s - loss: 0.9809 - acc: 0.7304 - val_loss: 1.5677 - val_acc: 0.6455
Epoch 31/100
 - 84s - loss: 0.9207 - acc: 0.7471 - val_loss: 1.4625 - val_acc: 0.6588
Epoch 32/100
 - 84s - loss: 0.9005 - acc: 0.7517 - val_loss: 1.5181 - val_acc: 0.6365
Epoch 33/100
 - 84s - loss: 0.8775 - acc: 0.7604 - val_loss: 1.5364 - val_acc: 0.6408
Epoch 34/100
 - 84s - loss: 0.8677 - acc: 0.7628 - val_loss: 1.5796 - val_acc: 0.6400
Epoch 35/100
 - 84s - loss: 0.8247 - acc: 0.7749 - val_loss: 1.4816 - val_acc: 0.6525
Epoch 36/100
 - 84s - loss: 0.8118 - acc: 0.7764 - val_loss: 1.5547 - val_acc: 0.6613
Epoch 37/100
 - 84s - loss: 0.7833 - acc: 0.7887 - val_loss: 1.5056 - val_acc: 0.6708
Epoch 38/100
 - 84s - loss: 0.7676 - acc: 0.7885 - val_loss: 1.4890 - val_acc: 0.6690
Epoch 39/100
 - 83s - loss: 0.7487 - acc: 0.7954 - val_loss: 1.5709 - val_acc: 0.6587
Epoch 40/100
 - 84s - loss: 0.7267 - acc: 0.8047 - val_loss: 1.5041 - val_acc: 0.6717
Epoch 41/100
 - 84s - loss: 0.7300 - acc: 0.8006 - val_loss: 1.5715 - val_acc: 0.6525
Epoch 42/100
 - 84s - loss: 0.6936 - acc: 0.8114 - val_loss: 1.3982 - val_acc: 0.6877
Epoch 43/100
 - 84s - loss: 0.7099 - acc: 0.8081 - val_loss: 1.5976 - val_acc: 0.6682
Epoch 44/100
 - 84s - loss: 0.6836 - acc: 0.8162 - val_loss: 1.5898 - val_acc: 0.6678
Epoch 45/100
 - 84s - loss: 0.6613 - acc: 0.8202 - val_loss: 1.3426 - val_acc: 0.6930
Epoch 46/100
 - 84s - loss: 0.6607 - acc: 0.8231 - val_loss: 1.6134 - val_acc: 0.6780
Epoch 47/100
 - 84s - loss: 0.6568 - acc: 0.8223 - val_loss: 1.3961 - val_acc: 0.6938
Epoch 48/100
 - 84s - loss: 0.6373 - acc: 0.8281 - val_loss: 1.4978 - val_acc: 0.6768
Epoch 49/100
 - 84s - loss: 0.6188 - acc: 0.8331 - val_loss: 1.5939 - val_acc: 0.6717
Epoch 50/100
 - 84s - loss: 0.6226 - acc: 0.8320 - val_loss: 1.6951 - val_acc: 0.6842
Epoch 51/100
 - 84s - loss: 0.6086 - acc: 0.8368 - val_loss: 1.5472 - val_acc: 0.6782
Epoch 52/100
 - 84s - loss: 0.6126 - acc: 0.8373 - val_loss: 1.7089 - val_acc: 0.6560
Epoch 53/100
 - 84s - loss: 0.5882 - acc: 0.8417 - val_loss: 1.6099 - val_acc: 0.6793
Epoch 54/100
 - 84s - loss: 0.5872 - acc: 0.8422 - val_loss: 1.6455 - val_acc: 0.6792
Epoch 55/100
 - 84s - loss: 0.5704 - acc: 0.8478 - val_loss: 1.5749 - val_acc: 0.6763
Epoch 56/100
 - 84s - loss: 0.5647 - acc: 0.8490 - val_loss: 1.5548 - val_acc: 0.6872
Epoch 57/100
 - 84s - loss: 0.5765 - acc: 0.8463 - val_loss: 1.6771 - val_acc: 0.6790
Epoch 58/100
 - 84s - loss: 0.5631 - acc: 0.8517 - val_loss: 1.6059 - val_acc: 0.6730
Epoch 59/100
 - 84s - loss: 0.5741 - acc: 0.8468 - val_loss: 1.5480 - val_acc: 0.6710
Epoch 60/100
 - 84s - loss: 0.5554 - acc: 0.8529 - val_loss: 1.6629 - val_acc: 0.6827
Epoch 61/100
 - 84s - loss: 0.5395 - acc: 0.8563 - val_loss: 1.6956 - val_acc: 0.6725
Epoch 62/100
 - 84s - loss: 0.5475 - acc: 0.8541 - val_loss: 1.6354 - val_acc: 0.6780
Epoch 63/100
 - 84s - loss: 0.5363 - acc: 0.8582 - val_loss: 1.4668 - val_acc: 0.6957
Epoch 64/100
 - 84s - loss: 0.5308 - acc: 0.8591 - val_loss: 1.7564 - val_acc: 0.6818
Epoch 65/100
 - 84s - loss: 0.5378 - acc: 0.8596 - val_loss: 1.5490 - val_acc: 0.6805
Epoch 66/100
 - 84s - loss: 0.5168 - acc: 0.8637 - val_loss: 1.5934 - val_acc: 0.6812
Epoch 67/100
 - 84s - loss: 0.5295 - acc: 0.8605 - val_loss: 1.6896 - val_acc: 0.6792
Epoch 68/100
 - 84s - loss: 0.5267 - acc: 0.8617 - val_loss: 1.7567 - val_acc: 0.6822
Epoch 69/100
 - 84s - loss: 0.5104 - acc: 0.8646 - val_loss: 1.6466 - val_acc: 0.6783
Epoch 70/100
 - 84s - loss: 0.5008 - acc: 0.8683 - val_loss: 1.5905 - val_acc: 0.6890
Epoch 71/100
 - 84s - loss: 0.4997 - acc: 0.8677 - val_loss: 1.7348 - val_acc: 0.6673
Epoch 72/100
 - 84s - loss: 0.5055 - acc: 0.8674 - val_loss: 1.6334 - val_acc: 0.6903
Epoch 73/100
 - 84s - loss: 0.5043 - acc: 0.8671 - val_loss: 1.7002 - val_acc: 0.6863
Epoch 74/100
 - 84s - loss: 0.4840 - acc: 0.8708 - val_loss: 1.5896 - val_acc: 0.6920
Epoch 75/100
 - 84s - loss: 0.4878 - acc: 0.8727 - val_loss: 1.6154 - val_acc: 0.6888
Epoch 76/100
 - 84s - loss: 0.4926 - acc: 0.8707 - val_loss: 1.5226 - val_acc: 0.6992
Epoch 77/100
 - 84s - loss: 0.4809 - acc: 0.8743 - val_loss: 1.6474 - val_acc: 0.6907
Epoch 78/100
 - 84s - loss: 0.4687 - acc: 0.8763 - val_loss: 1.6610 - val_acc: 0.6765
Epoch 79/100
 - 84s - loss: 0.4792 - acc: 0.8752 - val_loss: 1.6658 - val_acc: 0.6838
Epoch 80/100
 - 84s - loss: 0.4649 - acc: 0.8778 - val_loss: 1.5728 - val_acc: 0.6978
Epoch 81/100
 - 84s - loss: 0.4665 - acc: 0.8781 - val_loss: 1.5868 - val_acc: 0.7010
Epoch 82/100
 - 84s - loss: 0.4710 - acc: 0.8751 - val_loss: 1.7239 - val_acc: 0.6858
Epoch 83/100
 - 84s - loss: 0.4579 - acc: 0.8802 - val_loss: 1.6099 - val_acc: 0.6930
Epoch 84/100
 - 84s - loss: 0.4604 - acc: 0.8805 - val_loss: 1.7795 - val_acc: 0.6682
Epoch 85/100
 - 84s - loss: 0.4589 - acc: 0.8788 - val_loss: 1.5295 - val_acc: 0.6980
Epoch 86/100
 - 84s - loss: 0.4624 - acc: 0.8779 - val_loss: 1.9150 - val_acc: 0.6783
Epoch 87/100
 - 84s - loss: 0.4639 - acc: 0.8786 - val_loss: 1.6769 - val_acc: 0.6930
Epoch 88/100
 - 84s - loss: 0.4492 - acc: 0.8821 - val_loss: 1.6798 - val_acc: 0.6788
Epoch 89/100
 - 84s - loss: 0.4506 - acc: 0.8825 - val_loss: 1.7639 - val_acc: 0.6792
Epoch 90/100
 - 84s - loss: 0.4522 - acc: 0.8818 - val_loss: 1.5881 - val_acc: 0.6900
Epoch 91/100
 - 84s - loss: 0.4302 - acc: 0.8868 - val_loss: 1.7524 - val_acc: 0.6910
Epoch 92/100
 - 84s - loss: 0.4474 - acc: 0.8819 - val_loss: 1.6072 - val_acc: 0.7002
Epoch 93/100
 - 84s - loss: 0.4536 - acc: 0.8815 - val_loss: 1.8980 - val_acc: 0.6675
Epoch 94/100
 - 84s - loss: 0.4404 - acc: 0.8856 - val_loss: 1.6695 - val_acc: 0.6783
Epoch 95/100
 - 84s - loss: 0.4460 - acc: 0.8839 - val_loss: 1.5956 - val_acc: 0.6932
Epoch 96/100
 - 84s - loss: 0.4324 - acc: 0.8863 - val_loss: 1.5982 - val_acc: 0.7028
Epoch 97/100
 - 84s - loss: 0.4469 - acc: 0.8830 - val_loss: 1.7299 - val_acc: 0.6918
Epoch 98/100
 - 84s - loss: 0.4287 - acc: 0.8879 - val_loss: 1.7295 - val_acc: 0.6938
Epoch 99/100
 - 84s - loss: 0.4325 - acc: 0.8871 - val_loss: 1.7303 - val_acc: 0.6900
Epoch 100/100
 - 84s - loss: 0.4241 - acc: 0.8903 - val_loss: 1.8167 - val_acc: 0.6775
Reached validation accuracy is 0.6775000001589457
8411.163714408875
